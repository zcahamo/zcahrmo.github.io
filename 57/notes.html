---
layout: htmlpage
title: MATH0057 – Probability and Statistics
---

<a id="MATH0057-autopage-1"></a>
<div class="titlepage">


<div class="author">


<div class="oneauthor">

<p>
Based on lectures by Dr Kayvan Sadeghi<br />
Notes taken by Robert Moye
</p>
</div>

</div>


<div class="titledate">

<p>
Term 2 of 2021 – 2022
</p>
</div>

</div>

<p>
Probability is the quantification of uncertainty. Statistics is the interpretation of information under uncertainty.
</p>
<h4 id="autosec-3">Contents</h4>
<a id="MATH0057-autopage-3"></a>



<nav class="toc">

<p>
<a href="MATH0057.html#autosec-4" class="tocsection" >
<span class="sectionnumber">1</span>&#x2003;Probability</a>
</p>


<p>
<a href="MATH0057.html#autosec-5" class="tocsubsection" >
<span class="sectionnumber">1.1</span>&#x2003;Basic probability theory</a>
</p>


<p>
<a href="MATH0057.html#autosec-6" class="tocsubsubsection" >
<span class="sectionnumber">1.1.1</span>&#x2003;Definitions</a>
</p>


<p>
<a href="MATH0057.html#autosec-9" class="tocsubsubsection" >
<span class="sectionnumber">1.1.2</span>&#x2003;Sets</a>
</p>


<p>
<a href="MATH0057.html#autosec-13" class="tocsubsubsection" >
<span class="sectionnumber">1.1.3</span>&#x2003;Axioms</a>
</p>


<p>
<a href="MATH0057.html#autosec-18" class="tocsubsubsection" >
<span class="sectionnumber">1.1.4</span>&#x2003;Constructing probability functions</a>
</p>


<p>
<a href="MATH0057.html#autosec-22" class="tocsubsubsection" >
<span class="sectionnumber">1.1.5</span>&#x2003;Deductions</a>
</p>


<p>
<a href="MATH0057.html#autosec-25" class="tocsubsubsection" >
<span class="sectionnumber">1.1.6</span>&#x2003;Independence</a>
</p>


<p>
<a href="MATH0057.html#autosec-30" class="tocsubsubsection" >
<span class="sectionnumber">1.1.7</span>&#x2003;Conditional probability</a>
</p>


<p>
<a href="MATH0057.html#autosec-42" class="tocsubsection" >
<span class="sectionnumber">1.2</span>&#x2003;Discrete random variables</a>
</p>


<p>
<a href="MATH0057.html#autosec-43" class="tocsubsubsection" >
<span class="sectionnumber">1.2.1</span>&#x2003;Distributions</a>
</p>


<p>
<a href="MATH0057.html#autosec-56" class="tocsubsubsection" >
<span class="sectionnumber">1.2.2</span>&#x2003;Expectation</a>
</p>


<p>
<a href="MATH0057.html#autosec-68" class="tocsubsubsection" >
<span class="sectionnumber">1.2.3</span>&#x2003;Variance</a>
</p>


<p>
<a href="MATH0057.html#autosec-81" class="tocsubsubsection" >
<span class="sectionnumber">1.2.4</span>&#x2003;Probability generating function</a>
</p>


<p>
<a href="MATH0057.html#autosec-85" class="tocsubsection" >
<span class="sectionnumber">1.3</span>&#x2003;Standard discrete distributions</a>
</p>


<p>
<a href="MATH0057.html#autosec-86" class="tocsubsubsection" >
<span class="sectionnumber">1.3.1</span>&#x2003;Choosing a distribution</a>
</p>


<p>
<a href="MATH0057.html#autosec-93" class="tocsubsubsection" >
<span class="sectionnumber">1.3.2</span>&#x2003;Bernoulli</a>
</p>


<p>
<a href="MATH0057.html#autosec-95" class="tocsubsubsection" >
<span class="sectionnumber">1.3.3</span>&#x2003;Binomial</a>
</p>


<p>
<a href="MATH0057.html#autosec-101" class="tocsubsubsection" >
<span class="sectionnumber">1.3.4</span>&#x2003;Geometric</a>
</p>


<p>
<a href="MATH0057.html#autosec-108" class="tocsubsubsection" >
<span class="sectionnumber">1.3.5</span>&#x2003;Negative Binomial</a>
</p>


<p>
<a href="MATH0057.html#autosec-115" class="tocsubsubsection" >
<span class="sectionnumber">1.3.6</span>&#x2003;Hypergeometric</a>
</p>


<p>
<a href="MATH0057.html#autosec-120" class="tocsubsubsection" >
<span class="sectionnumber">1.3.7</span>&#x2003;Poisson</a>
</p>


<p>
<a href="MATH0057.html#autosec-126" class="tocsubsection" >
<span class="sectionnumber">1.4</span>&#x2003;Continuous random variables</a>
</p>


<p>
<a href="MATH0057.html#autosec-127" class="tocsubsubsection" >
<span class="sectionnumber">1.4.1</span>&#x2003;Probability density functions</a>
</p>


<p>
<a href="MATH0057.html#autosec-137" class="tocsubsubsection" >
<span class="sectionnumber">1.4.2</span>&#x2003;Expectation and varience</a>
</p>


<p>
<a href="MATH0057.html#autosec-142" class="tocsubsubsection" >
<span class="sectionnumber">1.4.3</span>&#x2003;Moment generating function</a>
</p>


<p>
<a href="MATH0057.html#autosec-145" class="tocsubsubsection" >
<span class="sectionnumber">1.4.4</span>&#x2003;Functions of random variables</a>
</p>


<p>
<a href="MATH0057.html#autosec-150" class="tocsubsection" >
<span class="sectionnumber">1.5</span>&#x2003;Standard continuous distributions</a>
</p>


<p>
<a href="MATH0057.html#autosec-151" class="tocsubsubsection" >
<span class="sectionnumber">1.5.1</span>&#x2003;Uniform</a>
</p>


<p>
<a href="MATH0057.html#autosec-160" class="tocsubsubsection" >
<span class="sectionnumber">1.5.2</span>&#x2003;Exponential</a>
</p>


<p>
<a href="MATH0057.html#autosec-171" class="tocsubsubsection" >
<span class="sectionnumber">1.5.3</span>&#x2003;Gamma</a>
</p>


<p>
<a href="MATH0057.html#autosec-186" class="tocsubsubsection" >
<span class="sectionnumber">1.5.4</span>&#x2003;Beta</a>
</p>


<p>
<a href="MATH0057.html#autosec-195" class="tocsubsubsection" >
<span class="sectionnumber">1.5.5</span>&#x2003;Normal</a>
</p>


<p>
<a href="MATH0057.html#autosec-207" class="tocsubsection" >
<span class="sectionnumber">1.6</span>&#x2003;Joint probability distributions</a>
</p>


<p>
<a href="MATH0057.html#autosec-208" class="tocsubsubsection" >
<span class="sectionnumber">1.6.1</span>&#x2003;Definition</a>
</p>


<p>
<a href="MATH0057.html#autosec-212" class="tocsubsubsection" >
<span class="sectionnumber">1.6.2</span>&#x2003;Discrete</a>
</p>
</nav>
<h4 id="autosec-4"><span class="sectionnumber">1&#x2003;</span>Probability</h4>
<a id="MATH0057-autopage-4"></a>
<h5 id="autosec-5"><span class="sectionnumber">1.1&#x2003;</span>Basic probability theory</h5>
<a id="MATH0057-autopage-5"></a>
<h6 id="autosec-6"><span class="sectionnumber">1.1.1&#x2003;</span>Definitions</h6>
<a id="MATH0057-autopage-6"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-7"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Sample space)</span>.   Probability statements, made in the context of an experiment, may result in a number of possible outcomes, each of which may be denoted
\(\omega \). The set of all possible outcomes is a sample space, denoted \(\Omega \).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-8"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Event)</span>.   A collection of outcomes is an event. An event \(E\) occurs if the outcome of the experiment of \(E\). \(E\subseteq \Omega \).
</p>

</li>

</ul>

</div>
<h6 id="autosec-9"><span class="sectionnumber">1.1.2&#x2003;</span>Sets</h6>
<a id="MATH0057-autopage-9"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-10"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.          For events \(E,F\), recall that
</p>
<ul style="list-style-type:none">


<li>
<p>
− \(E^c\) is the complement of \(E\), the set of outcomes that are not in \(E\).
</p>


</li>
<li>
<p>
− \(E\cup F\) is the union of \(E\) and \(F\), the set of outcomes in \(E\) or in \(F\) (or both).
</p>


</li>
<li>
<p>
− \(E\cap F\) is the intersection of \(E\) and \(F\), the set of outcomes in both \(E\) and \(F\).
</p>


</li>
<li>
<p>
− \(E\subseteq F\) denotes that \(E\) is a subset of \(F\), so every element of \(E\) is in \(F\).
</p>


</li>
<li>
<p>
− \(E\subset F\) denotes that \(E\) is a subset of \(F\), but \(E\) and \(F\) are not identical.
</p>


</li>
<li>
<p>
− \(E\backslash F=E\cap F^c\) is the set of elements of \(E\) that are not in \(F\).
</p>


</li>
<li>
<p>
− \(\emptyset =\{\}=\Omega ^c\) is the empty set, that which contains no elements.
</p>


</li>
<li>
<p>
− If \(E\cap F=\emptyset \) then no outcomes are common to both \(E\) and \(F\). These events are called disjoint, or mutually exclusive (m.e.)
</p>


</li>
<li>
<p>
− If all possible pairs of events \(E_1,\dots E_n\) are disjoint, they are called mutually disjoint or pairwise disjoint.
</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-11"></a>

<p>
<span class="amsthmnamedefinition">Law</span><span class="amsthmnumberdefinition"> <span class="textup">1.1</span></span><span class="amsthmnotedefinition"> (De Morgan’s)</span>.   For events \(E,F\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{0}\)</span>


<!--



                                                                                                              (E ∩ F )c = E c ∪ F c                                                                        (1.1)
                                                                                                                      c     c     c
                                                                                                              (E ∪ F ) = E ∩ F                                                                             (1.2)



-->


<p>


\begin{align}
(E\cap F)^c&amp;=E^c\cup F^c\\ (E\cup F)^c&amp;=E^c\cap F^c
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-12"></a>

<p>
<span class="amsthmnamedefinition">Law</span><span class="amsthmnumberdefinition"> <span class="textup">1.2</span></span><span class="amsthmnotedefinition"> (Distribution)</span>.   For events \(E,F,G\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{2}\)</span>


<!--



                                                                                                         E ∩ (F ∪ G) = (E ∩ F ) ∪ (E ∩ G)                                             (1.3)                   --><a id="eq:dis1"></a><!--
                                                                                                         E ∪ (F ∩ G) = (E ∪ F ) ∩ (E ∪ G)                                             (1.4)                   --><a id="eq:dis2"></a><!--



-->


<p>


\begin{align}
\label {eq:dis1}E\cap (F\cup G)&amp;=(E\cap F)\cup (E\cap G)\\ \label {eq:dis2}E\cup (F\cap G)&amp;=(E\cup F)\cap (E\cup G)
\end{align}


</p>

</li>

</ul>

</div>
<h6 id="autosec-13"><span class="sectionnumber">1.1.3&#x2003;</span>Axioms</h6>
<a id="MATH0057-autopage-13"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-14"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (\(\sigma \)-algebra event space)</span>.       For sample space \(\Omega \) and events \(E_i\in \Omega \) in an event space \(\mathcal {F}\), \(\mathcal {F}\) is a
\(\sigma \)-algebra if
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{4}\)</span>


<!--



                                                                                                                          Ω∈F                                                                                                                                  (1.5)
                                                                                                                   E ∈ F =⇒ E ∈ F c
                                                                                                                                                                                          (1.6)                                   --><a id="eq:axiom2"></a><!--
                                                                                                                                         n
                                                                                                                                        [
                                                                                                           E1 ∈ F , . . . , En ∈ F =⇒         Ei ∈ F                                      (1.7)                                   --><a id="eq:axiom3"></a><!--
                                                                                                                                        i=1



-->


<p>


\begin{gather}
\Omega \in \mathcal {F}\\ \label {eq:axiom2}E\in \mathcal {F}\implies E^c\in \mathcal {F}\\ \label {eq:axiom3}\displaystyle E_1\in \mathcal {F},\dots ,E_n\in \mathcal {F}\implies \bigcup _{i=1}^n E_i\in \mathcal {F}
\end{gather}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-15"></a>

<p>
<span class="amsthmnamedefinition">Example</span><span class="amsthmnumberdefinition"> <span class="textup">1.1</span></span>.   Consider an experiment with fair coin tossed once, which lands on heads or tails, \(H\) or \(T\). The sample space is \(\Omega
=\{H,T\}\) whilst the event space is \(\mathcal {F}=\big \{\emptyset ,\{H\},\{T\},\Omega \big \}\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-16"></a>

<p>
<span class="amsthmnamedefinition">Axiom</span><span class="amsthmnumberdefinition"> <span class="textup">1.3</span></span><span class="amsthmnotedefinition"> (Kolmogrov’s)</span>.   <a id="th:Kolmo"></a> A probability function \(P\) is a
mapping \(P:\mathcal {F}\to \R \) such that \(\forall E,F\in \mathcal {F}\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{7}\)</span>


<!--



                                                                                                                    P (E) ≥ 0                                           (1.8)                                         --><a id="eq:Kolmo1"></a><!--
                                                                                                                    P (Ω) = 1                                           (1.9)                                         --><a id="eq:Kolmo2"></a><!--
                                                                                                     E ∩ F = ∅ =⇒ P (E ∪ F ) = P (E) + P (F )                          (1.10)                                         --><a id="eq:Kolmo3"></a><!--



-->


<p>


\begin{gather}
P(E)\ge 0\label {eq:Kolmo1}\\ P(\Omega )=1\label {eq:Kolmo2}\\ E\cap F=\emptyset \implies P(E\cup F)=P(E)+P(F)\label {eq:Kolmo3}
\end{gather}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-17"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   For countably infinite sequences \(E_1,E_2,\dots \) of mutually disjoint sets in \(\mathcal {F}\), equation <span class="textup">(<a href="MATH0057.html#eq:Kolmo3">1.10</a>)</span> may need to be
adapted to
</p>

<p>
\[ P\left (\bigcup _{i=1}^\infty E_i\right )=\sum _{i=1}^\infty P(E_i) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<h6 id="autosec-18"><span class="sectionnumber">1.1.4&#x2003;</span>Constructing probability functions</h6>
<a id="MATH0057-autopage-18"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-19"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.        The axioms capture how probabilities behave intuitively, based on relative frequency (how many times an event occurs over many repetitions of an experiment), but allocation of useful probabilities rely on
mathematical modelling. All results will be deduced from the axioms.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-20"></a>

<p>
<span class="amsthmnamedefinition">Method</span>.    For a countable sample space, specify probabilities for each individual \(\omega \in \Omega \). For \(\Omega =\{w_1,w_2,\dots \}\), let \(\{p_1,p_2\}\) be a set of ‘weights’ satisfying
</p>

<p>
\[ p_k\ge 0,\quad \sum _{k=1}^\infty p_k=1 \]
</p>

<p>
Then for any event \(E\subseteq \Omega \),
</p>

<p>
\[ P(E)=\sum _{k:w_k\in E}p_k \]
</p>

<p>
Clearly this satisfies equations <span class="textup">(<a href="MATH0057.html#eq:Kolmo1">1.8</a>)</span> and <span class="textup">(<a href="MATH0057.html#eq:Kolmo2">1.9</a>)</span>. For equation <span class="textup">(<a
href="MATH0057.html#eq:Kolmo3">1.10</a>)</span>, see that
</p>

<p>
\[ P(E\cup F)=\sum _{i:\omega _i\in E\cup F}p_i=\sum _{i:w_i\in E}p_i+\sum _{j:w_j\in F}p_j=P(E)+P(F) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-21"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   For an uncountable sample space, construction of a probability function is more difficult, but the axioms still work.
</p>

</li>

</ul>

</div>
<h6 id="autosec-22"><span class="sectionnumber">1.1.5&#x2003;</span>Deductions</h6>
<a id="MATH0057-autopage-22"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-23"></a>

<p>
<span class="amsthmnamedefinition">Corollary</span><span class="amsthmnumberdefinition"> <span class="textup">1.4</span></span>.                       \(\forall E,F,E_1,\dots E_n\in \mathcal {F}\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{10}\)</span>


<!--



                                                                                                                      P (E c ) = 1 − P (E)                                         (1.11)                                                     --><a id="eq:cor1"></a><!--
                                                                                                                                P (E) ≤ 1                                          (1.12)                                                     --><a id="eq:cor2"></a><!--
                                                                                                         P (E ∪ F ) = P (E) + P (F ) − P (E ∩ F )                                  (1.13)                                                     --><a id="eq:cor4"></a><!--
                                                                                              E ⊆ F =⇒ P (F \E) = P (F ) − P (E)                       and    P (E) ≤ P (F )       (1.14)                                                     --><a id="eq:cor3"></a><!--
                                                                                                                          n
                                                                                                                                     !        n
                                                                                                                          [                   X
                                                                                                                 P              Ei        ≤         P (Ei )                        (1.15)                                                     --><a id="eq:cor5"></a><!--
                                                                                                                          i=1                 i=1



-->


<p>


\begin{gather}
\label {eq:cor1} P(E^c)=1-P(E)\\ \label {eq:cor2} P(E)\le 1\\ \label {eq:cor4} P(E\cup F)=P(E)+P(F)-P(E\cap F)\\ \label {eq:cor3} E\subseteq F\implies P(F\backslash E)=P(F)-P(E)\quad \text {and}\quad P(E)\le P(F)\\ \label {eq:cor5} P\left
(\bigcup _{i=1}^n E_i\right )\le \sum _{i=1}^n P(E_i)
\end{gather}


</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{15}\)</span>


<!--


                                                                                                           n
                                                                                                                      !         n
                                                                                                           [                    X
                                                                                                     P           Ei       =               P (Ei )
                                                                                                           i=1                  i=1
                                                                                                                                 X
                                                                                                                           −              P (Ei ∩ Ej )
                                                                                                                                 i<j
                                                                                                                                 X                                                                (1.16)                                      --><a id="eq:cor6"></a><!--
                                                                                                                           +              P (Ei ∩ Ej ∩ Ek )
                                                                                                                                i<j<k

                                                                                                                                     ..
                                                                                                                                      .
                                                                                                                           + (−1)n−1 P (E1 ∩ · · · ∩ En )


-->


<p>


\begin{align}
\label {eq:cor6} \begin{split} P\left (\bigcup _{i=1}^n E_i\right )=\!\!&amp;\phantom {-}\:\sum _{i=1}^n\:       P(E_i)\\ &amp;-\:\sum _{i&lt;j}\:                       P(E_i\cap E_j)\\ &amp;+\!\sum _{i&lt;j&lt;k}\!   P(E_i\cap E_j\cap E_k)\\ &amp;\phantom {-}\ \ \
\ \vdots \\ &amp;+(-1)^{n-1}P(E_1\cap \dots \cap E_n) \end {split}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-24"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:cor1">1.11</a>)</span>. By equations <span class="textup">(<a href="MATH0057.html#eq:Kolmo2">1.9</a>)</span> and <span class="textup">(<a href="MATH0057.html#eq:Kolmo3">1.10</a>)</span>,
</p>
<p>
\[ P(E)+P(E^c)=P(E\cup E^c)=P(\Omega )=1 \]
</p>
</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:cor2">1.12</a>)</span>. By equations <span class="textup">(<a href="MATH0057.html#eq:Kolmo1">1.8</a>)</span> and <span class="textup">(<a href="MATH0057.html#eq:cor1">1.11</a>)</span>,
</p>
<p>
\[ P(E)\le P(E)+P(E^c)=1 \]
</p>
</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span>. By equations <span class="textup">(<a href="MATH0057.html#eq:Kolmo1">1.8</a>)</span> and <span class="textup">(<a href="MATH0057.html#eq:cor1">1.11</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--

                                                                                                                                                                
                                                                                                                         P (E ∪ F ) = P E ∪ (F ∩ E c )

                                                                                                                                       = P (E) + P (F ∩ E c )
                                                                                                                                       = P (E) + P (F cc ∩ E c )
                                                                                                                                       = P (E) + P (F c ∪ E)c
                                                                                                                                       = P (E) + 1 − P (F c ∪ E)
                                                                                                                                                                               
                                                                                                                                       = P (E) + 1 − P F c ∪ (E ∩ F )

                                                                                                                                       = P (E) + 1 − P (F c ) − P (E ∩ F )
                                                                                                                                       = P (E) + P (F ) − P (E ∩ F )


-->


<p>


\begin{align*}
P(E\cup F)&amp;=P\big (E\cup (F\cap E^c)\big )\\ &amp;=P(E)+P(F\cap E^c)\\ \iffalse &amp;=P(E)+P(F\cap (E^c\cup F^c))\\ &amp;=P(E)+P\big (F\cap E^c)\cup (F\cap F^c)\big )\\ &amp;=P(E)+P\big (F^c\cup E)^c\cup (F^c\cap F)^c\big )\\
&amp;=P(E)+P\big (F^c\cup E)\cap (F^c\cup F)\big )^c\\ &amp;=P(E)+1-P\big (F^c\cup E)\cap (F^c\cup F)\big )\\ \fi &amp;=P(E)+P(F^{cc}\cap E^c)\\ &amp;=P(E)+P(F^c\cup E)^c\\ &amp;=P(E)+1-P(F^c\cup E)\\ &amp;=P(E)+1-P\big (F^c\cup (E\cap
F)\big )\\ &amp;=P(E)+1-P(F^c)-P(E\cap F)\\ &amp;=P(E)+P(F)-P(E\cap F)
\end{align*}


</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:cor3">1.14</a>)</span>. Since \(E\subseteq F\), \(F^c\subseteq E^c\), so \(F\cup E^c=\Omega \). So by equations <span class="textup">(<a href="MATH0057.html#eq:cor1">1.11</a>)</span> and <span
class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                                                                 P (F \E) = P (F ∩ E c )
                                                                                                                                              = P (F ) + P (E c ) − P (F ∪ E c )
                                                                                                                                              = P (F ) + 1 − P (E c ) − P (Ω)
                                                                                                                                              = P (F ) − P (E)
By equation (1.8),
                                                                                                                P (E) + P (F \E) = P (F )
                                                                                                                                     P (E) ≤ P (F )


-->


<p>


\begin{align*}
P(F\backslash E)&amp;=P(F\cap E^c)\\ &amp;=P(F)+P(E^c)-P(F\cup E^c)\\ &amp;=P(F)+1-P(E^c)-P(\Omega )\\ &amp;=P(F)-P(E) \shortintertext {By \cref {eq:Kolmo1},} P(E)+P(F\backslash E)&amp;=P(F)\\ P(E)&amp;\le P(F)
\end{align*}


</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:cor5">1.15</a>)</span>. (by induction) The case \(n=1\) is obvious (and \(n=2\) is given by equation <span class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span>). Let
</p>
<p>
\[ U_r=\bigcup _{i=1}^r E_i \]
</p>
<p>
Suppose true for \(n=k\). Then
</p>
<p>
\[ P\left (U_k\right )\le \sum _{i=1}^k P(E_i) \]
</p>
<p>
By equation <span class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                                               P (Uk+1 ) = P (Uk ∪ Er+1 )
                                                                                                                                 = P (Uk ) + P (Ek+1 ) − P (Uk ∩ E k + 1)
By equation (1.8),
                                                                                                                                 ≤ P (Uk ) + P (Ek+1 )
                                                                                                                                     k
                                                                                                                                     X
                                                                                                                                 =           P (Ei ) + P (Ek+1 )
                                                                                                                                       i=1
                                                                                                                                     k+1
                                                                                                                                     X
                                                                                                               P (Uk+1 ) ≤                   P (Ei )
                                                                                                                                       i=1


-->


<p>


\begin{align*}
P(U_{k+1})&amp;=P(U_k\cup E_{r+1})\\ &amp;=P(U_k)+P(E_{k+1})-P(U_k\cap E^k+1) \shortintertext {By \cref {eq:Kolmo1},} &amp;\le P(U_k)+P(E_{k+1})\\ &amp;=\sum _{i=1}^k P(E_i)+P(E_{k+1})\\ P(U_{k+1})&amp;\le \sum _{i=1}^{k+1} P(E_i)
\end{align*}
So true for \(n=k+1\).
</p>
<p>
True for \(n=1\), and true for \(n=k\implies \) true for \(n=k+1\), so by induction, true for \(n\in \N \).
</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:cor6">1.16</a>)</span>. (by induction) The case \(n=2\) is given by equation <span class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span>. As before, let
</p>
<p>
\[ U_r=\bigcup _{i=1}^r E_i \]
</p>
<p>
Suppose true for \(n=k\). Then by equation <span class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span>,
</p>
<p>
\[ P(U_{k+1})=P(U_r\cup E_{k+1})=P(U_k)+P(E_{k+1})-P(U_k\cap E_{k+1}) \]
</p>
<p>
By equation <span class="textup">(<a href="MATH0057.html#eq:dis1">1.3</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                                                  Uk ∩ Ek+1 = (E1 ∪ · · · ∪ Ek ) ∩ Ek+1
                                                                                                                                       = (E1 ∩ Ek+1 ) ∪ · · · ∪ (Ek ∪ Ek+1 )
                                                                                                                                           k                        k
                                                                                                                                           X                        X
                                                                                                                                       =         Ei ∩ Ek+1 =:             Ii
                                                                                                                                           i=1                      i=1


-->


<p>


\begin{align*}
U_k\cap E_{k+1}&amp;=(E_1\cup \dots \cup E_k)\cap E_{k+1}\\ &amp;=(E_1\cap E_{k+1})\cup \dots \cup (E_k\cup E_{k+1})\\ &amp;=\sum _{i=1}^k E_i\cap E_{k+1}=:\sum _{i=1}^kI_i
\end{align*}
By the assumption,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                                                                 k
                                                                                                                                 X
                                                                                                               P (Uk ) =               P (Ei ) − · · · + (−1)r−1 P (E1 ∩ · · · ∩ Ek+1 )
                                                                                                                                 i=1
                                                                                                              k
                                                                                                                         !       k
                                                                                                              [                  X
                                                                                                         P          Ii       =         P (Ii ) − · · · + (−1)r−1 P (I1 ∩ · · · ∩ Ik+1 )
                                                                                                              i=1                i=1




-->


<p>


\begin{align*}
P(U_k)&amp;=\sum _{i=1}^k P(E_i)-\dots +(-1)^{r-1}P(E_1\cap \dots \cap E_{k+1})\\ P\left (\bigcup _{i=1}^kI_i\right )&amp;=\sum _{i=1}^k P(I_i)-\dots +(-1)^{r-1}P(I_1\cap \dots \cap I_{k+1})\\
\end{align*}
So
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                                     P (Uk+1 ) = P (Uk ) + P (Ek+1 ) − P (Ik )
                                                                                                                  k
                                                                                                                  X
                                                                                                              =            P (Ei ) − · · · + (−1)r−1 P (E1 ∩ · · · ∩ Ek+1 ) + P (Ek+1 )
                                                                                                                    i=1
                                                                                                                           k
                                                                                                                                                                                        !
                                                                                                                           X
                                                                                                                                                         r−1
                                                                                                                −                P (Ii ) − · · · + (−1)        P (I1 ∩ · · · ∩ Ik+1 )
                                                                                                                           i=1
                                                                                                                  k+1
                                                                                                                  X
                                                                                                              =            P (Ek+1 )
                                                                                                                    i=1
                                                                                                                           X
                                                                                                                −                    P (Ei ∩ Ej ) + · · · + (−1)r−1 P (E1 ∩ · · · ∩ Ek+1 )
                                                                                                                         1≤i<j≤k
                                                                                                                           k
                                                                                                                                                                                        !
                                                                                                                           X
                                                                                                                −                P (Ii ) − · · · + (−1)r−1 P (I1 ∩ · · · ∩ Ik+1 )
                                                                                                                           i=1

Substitute back \(I_i=E_i\cap E_{k+1}\), and pair up and combine terms from the two lines.
                                                                                                                  k+1
                                                                                                                  X
                                                                                                              =            P (Ek+1 )
                                                                                                                    i=1
                                                                                                                                                          k
                                                                                                                                                                                   !
                                                                                                                             X                            X
                                                                                                                −                       P (Ei ∩ Ej ) +          P (Ei ∩ Ek+1 )
                                                                                                                           1≤i<j≤k                        i=1
                                                                                                                                                                                            !
                                                                                                                                 X                                        X
                                                                                                                +                            P (Ei ∩ Ej ∩ El ) +               P (Ii ∩ Ij )
                                                                                                                           1≤i<j<l≤k                                 1≤i<j≤k

                                                                                                                    ..
                                                                                                                     .
                                                                                                                          k+1
                                                                                                                          X                       X
                                                                                                    P (Uk+1 ) =                  P (Ei ) −                 P (Ei ∩ Ej ) + . . .
                                                                                                                          i=1                 1≤i<j≤k+1

                                                                                                                · · · + (−1)k P (E1 ∩ · · · ∩ Ek+1 )


-->


<p>


\begin{align*}
P(U_{k+1})\!&amp;=P(U_k)+P(E_{k+1})-P(I_k)\\ &amp;=\!\sum _{i=1}^k P(E_i)-\dots +(-1)^{r-1}P(E_1\cap \dots \cap E_{k+1})+P(E_{k+1})\\ &amp;\phantom {=}-\left (\sum _{i=1}^k P(I_i)-\dots +(-1)^{r-1}P(I_1\cap \dots \cap I_{k+1})\right )\\
&amp;=\!\sum _{i=1}^{k+1}P(E_{k+1})\\ &amp;\phantom {=}-\sum _{1\le i&lt;j\le k}P(E_i\cap E_j)+\dots +(-1)^{r-1}P(E_1\cap \dots \cap E_{k+1})\\ &amp;\phantom {=}-\left (\sum _{i=1}^k P(I_i)-\dots +(-1)^{r-1}P(I_1\cap \dots \cap
I_{k+1})\right ) \intertext {Substitute back $I_i=E_i\cap E_{k+1}$, and pair up and combine terms from the two lines.} &amp;=\!\sum _{i=1}^{k+1}P(E_{k+1})\\ &amp;\phantom {=}-\left (\sum _{1\le i&lt;j\le k}P(E_i\cap E_j)+\sum _{i=1}^k
P(E_i\cap E_{k+1})\right )\\ &amp;\phantom {=}+\left (\sum _{1\le i&lt;j&lt;l\le k}P(E_i\cap E_j\cap E_l)+\sum _{1\le i&lt;j\le k}P(I_i\cap I_j)\right )\\ &amp;\phantom {=}\;\;\vdots \\ P(U_{k+1})&amp;=\phantom {-}\:\sum _{i=1}^{k+1}\:
P(E_i)-\sum _{1\le i&lt;j\le k+1}\:       P(E_i\cap E_j)+\dots \\ &amp;\phantom {=}\dots +(-1)^{k}P(E_1\cap \dots \cap E_{k+1})
\end{align*}
Hence true for \(n=k+1\).
</p>
<p>
True for \(n=2\), and true for \(n=k\implies \) true for \(n=k+1\), so by induction, true for \(n\in \Z ,\ n\ge 2\).
</p>
</li>
</ul>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<h6 id="autosec-25"><span class="sectionnumber">1.1.6&#x2003;</span>Independence</h6>
<a id="MATH0057-autopage-25"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-26"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Independent)</span>.   Events \(E\) and \(F\) are independent if
</p>

<p>
\[ P(E\cap F)=P(E)P(F) \]
</p>

<p>
Events \(E_1,\dots ,E_n\) are mutually independent if, for any collection \(E_{i_1},\dots ,E_{i_k}\),
</p>

<p>
\[ P(E_{i_1}\cap \dots E_{i_k})=\prod _{j=1}^k P(E_{i_j}) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-27"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Independence captures the idea that events are unrelated, that one does not affect the other.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-28"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   For independent events \(E,F\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--



                                                                                            Mutually exclusive ⇐⇒ P (E ∩ F ) = 0 ⇐⇒ P (E)P (F ) = 0
                                                                                                              ⇐⇒ P (E) = 0 or P (F ) = 0



-->


<p>


\begin{align*}
\text {Mutually exclusive}&amp;\iff P(E\cap F)=0\iff P(E)P(F)=0\\ &amp;\iff P(E)=0\text { or }P(F)=0
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-29"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Mutual independence is a stronger condition than pairwise independence, which only requires pairs of events \(E_i,E_j\) to be independent.
</p>

</li>

</ul>

</div>
<h6 id="autosec-30"><span class="sectionnumber">1.1.7&#x2003;</span>Conditional probability</h6>
<a id="MATH0057-autopage-30"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-31"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Conditional probability)</span>.   The conditional probability of \(F\) given \(E\) is the probability that \(F\) occurs when \(E\) is known to have occurred. For
\(P(E)&gt;0\), it is given by
</p>

<p>
\[ P(F|E)=\frac {P(E\cap F)}{P(E)} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-32"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The definition of conditional probability may be interpreted as changing the sample space from \(\Omega \) to \(E\), that is \(P(F|E)\) is the probability of \(F\) given that the outcome of the experiment is
known to be in \(E\). Note that \(P(E|E)=1\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-33"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.         If \(E\) and \(F\) are independent, and \(P(E)\ne 0\), then
</p>

<p>
\[ P(F|E)=\frac {P(E\cap F)}{P(E)}=\frac {P(E)P(F)}{P(E)}=P(F) \]
</p>

<p>
So \(E\) occurring does not change the chance of \(F\) occurring.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-34"></a>

<p>
<span class="amsthmnamedefinition">Law</span><span class="amsthmnumberdefinition"> <span class="textup">1.5</span></span><span class="amsthmnotedefinition"> (Total probability)</span>.   <a id="thm:tot"></a> Let \(\{E_i\}\) be a partition of
\(\Omega \) (the \(E_i\)s are disjoint, and their union gives \(\Omega \)). Then for any event \(F\),
</p>

<p>
\[ P(F)=\sum _i P(F|E_i)P(E_i) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-35"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> By induction on equation <span class="textup">(<a href="MATH0057.html#eq:dis1">1.3</a>)</span>,
</p>

<p>
\[ F=F\cap \Omega =F\cap \left (\bigcup _iE_i\right )=\bigcup _iE_i\cap F \]
</p>

<p>
So
</p>

<p>
\[ P(F)=\sum _i P(F\cap E_i)=\sum _i P(F|E_i)P(E_i)\qedhere \]
</p>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-36"></a>

<p>
<span class="amsthmnamedefinition">Theorem</span><span class="amsthmnumberdefinition"> <span class="textup">1.6</span></span><span class="amsthmnotedefinition"> (Bayes’)</span>.   <a id="thm:bay"></a> Let \(\{E_i\}\) be a partition of \(\Omega
\) and \(F\) be any event with \(P(F)&gt;0\). Then
</p>

<p>
\[ P(E_i|F)=\frac {P(F|E_i)P(E_i)}{P(F)}=\frac {P(F|E_i)P(E_i)}{\sum _i P(F|E_i)P(E_i)} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-37"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> By definition of conditional probability
</p>

<p>
\[ P(E_i|F)=\frac {P(E_i\cap F)}{P(F)}\quad \text {and}\quad P(E_i\cap F)=P(F|E_i)P(E_i) \]
</p>

<p>
The denominator comes from the Law of Total Probability (<a href="MATH0057.html#thm:tot">1.5</a>). <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-38"></a>

<p>
<span class="amsthmnamedefinition">Example</span><span class="amsthmnumberdefinition"> <span class="textup">1.2</span></span>.   An urn contains five red balls and one white ball. A ball is drawn and then it and another ball of the same colour are placed
back in the urn. Finally a second ball is drawn.
</p>
<ul style="list-style-type:none">


<li>
<p>
1. What is the probability that the second ball is white?
</p>


</li>
<li>
<p>
2. If the second ball is white, what is the probability that the first was red?
</p>
</li>
</ul>

<p>
Let \(W_i\) be the probability the \(i^\text {th}\) ball is white, and \(R_i\) the corresponding probability for red.
</p>
<ul style="list-style-type:none">


<li>
<p>
1. By the Law of Total Probability (<a href="MATH0057.html#thm:tot">1.5</a>),
</p>
<p>
\[ \displaystyle P(W_2)=P(W_2|R_1)P(R)+P(W_2|W_1)P(W_1)=\frac 17\frac 56+\frac 27\frac 16=\frac 16 \]
</p>
</li>
<li>
<p>
2. By Bayes’ Theorem (<a href="MATH0057.html#thm:bay">1.6</a>),
</p>
<p>
\[ \displaystyle P(R_1|W_2)=\frac {P(W_2|R_1)P(R_1)}{P(W_2)}=\frac {\frac 17\frac 56}{\frac 16}=\frac 57 \]
</p>
<p>


</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-39"></a>

<p>
<span class="amsthmnamedefinition">Law</span><span class="amsthmnumberdefinition"> <span class="textup">1.7</span></span><span class="amsthmnotedefinition"> (Generalised Multiplication)</span>.   <a id="thm:genmult"></a> For events \(E_1,\dots
,E_n\) that satisfy
</p>

<p>
\[ P(E_1\cap \dots \cap E_{n-1})&gt;0, \]
</p>

<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                                n
                                                                                                           !       n
                                                                                                \                  Y
                                                                                           P          Ei       =         P (Ei |E1 ∩ · · · ∩ Ei−1 )
                                                                                                i=1                i=1

                                                                                      P (E1 ∩ · · · ∩ En ) = P (E1 )P (E2 |E1 )P (E3 |E1 ∩ E2 ) . . . P (En |E1 ∩ · · · ∩ En−1 )



-->


<p>


\begin{align*}
P\left (\bigcap _{i=1}^nE_i\right )&amp;=\prod _{i=1}^n P(E_i|E_1\cap \dots \cap E_{i-1})\\ P(E_1\cap \dots \cap E_{n})&amp;=P(E_1)P(E_2|E_1)P(E_3|E_1\cap E_2)\dots P(E_n|E_1\cap \dots \cap E_{n-1})
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-40"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> All conditional probabilities are defined, since equation <span class="textup">(<a href="MATH0057.html#eq:cor4">1.13</a>)</span> gives
</p>

<p>
\[ P(E_1)\ge P(E_1\cap E_2)\ge \dots \ge P(E_1\cap \dots \cap E_{n-1})&gt;0 \]
</p>

<p>
By the definition of conditional probability,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--


                                                                                              n                                    n
                                                                                              Y                                    Y P (E1 ∩ · · · ∩ Ei )
                                                                                                    P (Ei |E1 ∩ · · · ∩ Ei−1 ) =
                                                                                                                                         P (E1 ∩ · · · ∩ Ei−1 )
                                                                                              i=1                                  i=1
                                                                                                                                   n                            n−1
                                                                                                                                   Y                            Y            1
                                                                                                                               =         P (E1 ∩ · · · ∩ Ei )
                                                                                                                                                                      P (E1 ∩ · · · ∩ Ei )
                                                                                                                                   i=1                          i=1
                                                                                                                                   n−1                                n
                                                                                                                                                                                 !
                                                                                                                                   Y P (E1 ∩ · · · ∩ Ei )             \
                                                                                                                               =                                P           Ei
                                                                                                                                         P (E1 ∩ · · · ∩ Ei )
                                                                                                                                   i=1                                i=1
                                                                                                                                         n
                                                                                                                                                    !
                                                                                                                                         \
                                                                                                                               =P              Ei
                                                                                                                                         i=1



-->


<p>


\begin{align*}
\prod _{i=1}^n P(E_i|E_1\cap \dots \cap E_{i-1})&amp;=\prod _{i=1}^n\frac {P(E_1\cap \dots \cap E_i)}{P(E_1\cap \dots \cap E_{i-1})}\\ &amp;=\prod _{i=1}^nP(E_1\cap \dots \cap E_i)\prod _{i=1}^{n-1}\frac {1}{P(E_1\cap \dots \cap E_{i})}\\
&amp;=\prod _{i=1}^{n-1} \frac {P(E_1\cap \dots \cap E_i)}{P(E_1\cap \dots \cap E_i)}P\left (\bigcap _{i=1}^nE_i\right )\\ &amp;=P\left (\bigcap _{i=1}^nE_i\right )
\end{align*}
or written out,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--



                                                                                                                           P (E1 ∩ E2 ) P (E1 ∩ E2 ∩ E3 )      P (E1 ∩ E2 ∩ En )
                                                                                          P (E1 ∩ · · · ∩ En ) = P (E1 )                                  ...
                                                                                                                              P (E1 )      P (E1 ∩ E2 )       P (E1 ∩ E2 ∩ En−1 )
                                                                                                              = P (E1 )P (E2 |E1 )P (E3 |E1 ∩ E2 ) . . . P (En |E1 ∩ · · · ∩ En−1 )



-->


<p>


\begin{align*}
P(E_1\cap \dots \cap E_n)&amp;=P(E_1)\frac {P(E_1\cap E_2)}{P(E_1)}\!\frac {P(E_1\cap E_2\cap E_3)}{P(E_1\cap E_2)}\dots \frac {P(E_1\cap E_2\cap E_n)}{P(E_1\cap E_2\cap E_{n-1})}\\ &amp;=P(E_1)P(E_2|E_1)P(E_3|E_1\cap E_2)\dots
P(E_n|E_1\cap \dots \cap E_{n-1})
\end{align*}
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-41"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   This may be useful for problems where an event of interest arises from a sequence of contributing events.
</p>

</li>

</ul>

</div>
<h5 id="autosec-42"><span class="sectionnumber">1.2&#x2003;</span>Discrete random variables</h5>
<a id="MATH0057-autopage-42"></a>
<h6 id="autosec-43"><span class="sectionnumber">1.2.1&#x2003;</span>Distributions</h6>
<a id="MATH0057-autopage-43"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-44"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Discrete r.v.)</span>.           For a countable sample space \(\Omega \), a function
</p>

<p>
\[ X:\Omega \to \R \]
</p>

<p>
is called a discrete random variable (discrete r.v.). This maps outcomes of an experiment to numbers. For brevity, its usage may be notated
</p>

<p>
\[ P\big (\{\omega :X(\omega )=x\}\big )=P\big (X(\omega )=x\big )=P(X=x) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-45"></a>

<p>
<span class="amsthmnamedefinition">Example</span><span class="amsthmnumberdefinition"> <span class="textup">1.3</span></span>.   When tossing a coin twice, the number of heads obtained may be given by the function
</p>

<p>
\[X=\big \{(HH,2),(HT,1),(TH,1),(TT,0)\big \}\]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-46"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Probability mass function)</span>.   For a discrete random variable \(X\) taking values in the set \(\{x_i\}\), the function \(p_X(x)=P(X=x)\) is the probability mass
function (pmf) of \(X\).
</p>

<p>
If unambiguous, this may be written \(p_X(x)=p(x)\). For integers, this may be written \(p(X=n)=p_n\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-47"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.      For a collection \(\{p(x_i)\}\) to be a pmf, it must satisfy the axioms of probability (<a href="MATH0057.html#th:Kolmo">1.3</a>).
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:Kolmo1">1.8</a>)</span>. Since the \(p\)s are probabilities of events, \(\forall i\ p(x_i)\ge 0\).
</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:Kolmo2">1.9</a>)</span>. Since the sample space may be written as a union of disjoint events
</p>
<p>
\[ \Omega =\bigcup _i\big \{\omega :X(\omega )=x_i\big \} \]
</p>
<p>
by equation <span class="textup">(<a href="MATH0057.html#eq:Kolmo3">1.10</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--

                                                                                                                                                       !
                                                                                                         X                     [
                                                                                                              p(xi ) = P               ω : X(ω) = xi
                                                                                                          i                    i
                                                                                                                       X                              
                                                                                                                   =           P       ω : X(ω) = xi       = P (Ω) = 1
                                                                                                                           i


-->


<p>


\begin{align*}
\sum _i p(x_i)&amp;=P\left (\bigcup _i\big \{\omega :X(\omega )=x_i\big \}\right )\\ &amp;=\sum _i P\big (\big \{\omega :X(\omega )=x_i\big \}\big )=P(\Omega )=1
\end{align*}


</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:Kolmo3">1.10</a>)</span>. Satisfied by the selection of mutually independent events \(\{x_i\}\).
</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-48"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The collection \(\{x_i\}\) may be regarded as a sample space in its own right, with the probability function \(P\big (\{x_i\}\big )=p(x_i.)\). This may be appropriate to use when only the values taken by
\(X\) are of interest.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-49"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.   It may be useful to consider the probability that \(X\) lies within an interval.
</p>

<p>
\[ P(a\le X\le b)=P\big (\{\omega :a\le X(\omega )\le b\}\big )=\sum _{i:a\le x_i\le b} p_x(x_i) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-50"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (CDF)</span>.   For any random variable \(X\), the (Cumulative) distribution function (cdf) of \(X\) is
</p>

<p>
\[ F_X(x)=P(X\le x)=\sum _{i:x_i\le x} p_X(x_i) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-51"></a>

<p>
<span class="amsthmnamedefinition">Rule</span><span class="amsthmnumberdefinition"> <span class="textup">1.8</span></span>.
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{16}\)</span>


<!--



                                                                                                                F (x) → 0    as    x → −∞                                                                                         (1.17)
                                                                                                                 F (x) → 1    as   x→∞                                                                                            (1.18)
                                                                                                              F (x) is monotonic increasing                                                                                       (1.19)
                                                                                          F (x) is a step function, with discontinuities at {xi } of height p(xi )                                                                (1.20)



-->


<p>


\begin{gather}
F(x)\to 0\quad \text {as}\quad x\to -\infty \\ F(x)\to 1\quad \text {as}\quad x\to \infty \\ F(x)\ \text {is monotonic increasing}\\ F(x)\ \text {is a step function, with discontinuities at }\{x_i\}\text { of height }p(x_i)
\end{gather}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-52"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Obvious from the axioms of probability (<a href="MATH0057.html#th:Kolmo">1.3</a>). <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-53"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.        The pmf may be derived from the cdf as
</p>

<p>
\[ p(x)=P(X=x)=P(X\le x)-P(X&lt;x)=\lim _{\delta \to 0^+}\big (F(x)-F(x-\delta )\big ) \]
</p>

<p>
A general version of this result follows in rule <a href="MATH0057.html#thm:disint">1.9</a>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-54"></a>

<p>
<span class="amsthmnamedefinition">Rule</span><span class="amsthmnumberdefinition"> <span class="textup">1.9</span></span>.   <a id="thm:disint"></a>
</p>

<p>
\[ a&lt;b\implies P(a&lt;X\le b)=F(b)-F(a) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-55"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> By equation <span class="textup">(<a href="MATH0057.html#eq:Kolmo3">1.10</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{20}\)</span>


<!--



                                                                                                          F (b) = P (X ≤ b)
                                                                                                                                         
                                                                                                               = P (X ≤ a) ∪ (a < X ≤ b)

                                                                                                               = P (X ≤ a) + P (a < X ≤ b)
                                                                                                               = F (a) + P (a < X ≤ b)



-->


<p>


\begin{align*}
F(b)&amp;=P(X\le b)\\ &amp;=P\big ((X\le a)\cup (a&lt;X\le b)\big )\\ &amp;=P(X\le a)+P(a&lt;X\le b)\\ &amp;=F(a)+P(a&lt;X\le b)\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<h6 id="autosec-56"><span class="sectionnumber">1.2.2&#x2003;</span>Expectation</h6>
<a id="MATH0057-autopage-56"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-57"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Median)</span>.           The median of a distribution is
</p>

<p>
\[ x:F(x)=\frac 12 \]
</p>

<p>
If repeating an experiment, a random variable would be expected to lie in equal proportions above and below the median.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-58"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Mode)</span>.   The mode is the value with highest probability.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-59"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Expectation)</span>.   The expectation, or expected value, or (population) mean of a discrete random variable \(X\) is
</p>

<p>
\[ E(X):=\sum _i x_i P(X=x_i)=\sum _i x_ip(x_i) \]
</p>

<p>
for probability mass function \(p\), provided the sum is well-defined.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-60"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   In the countably-infinite case, the sum is not guaranteed to converge. \(E(X)\) exists if the sum converges absolutely, that is, if
</p>

<p>
\[ \sum _i\abs {x_i}P(X=x_i)&lt;\infty \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-61"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   \(E(X)\) represents an idealised long-run average for the values of \(X\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-62"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Expected value may be calculated using the values taken by \(X\), or directly on the sample space.
</p>

<p>
\[ E(x)=\sum _ix_iP\{\omega :X(\omega )=x_i\}=\sum _{\omega \in \Omega } X(\omega )P({\omega }) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-63"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.10</span></span><span class="amsthmnotedefinition"> (Expectation of a function)</span>.   For a random variable \(X\) and \(\phi :\R
\to \R \), \(\phi (X)\) is a random variable \(\phi (X):\Omega \to \R \), so its expectation may be calculated
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{20}\)</span>


<!--


                                                                                                                             X              
                                                                                                                 E ϕ(X) =           ϕ X(ω) P ({ω})
                                                                                                                              ω∈Ω
                                                                                                                              X
                                                                                                                          =         ϕ(xi )P ({ω : X(w) = xi })
                                                                                                                               i
                                                                                                                              X
                                                                                                                          =         ϕ(xi )pX (xi )
                                                                                                                               i



-->


<p>


\begin{align*}
E\big (\phi (X)\big )&amp;=\sum _{\omega \in \Omega } \phi \big (X(\omega )\big )P(\{\omega \})\\ &amp;=\sum _{i} \phi ({x_i})P(\{\omega :X(w)=x_i\})\\ &amp;=\sum _{i} \phi ({x_i})p_X(x_i)
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-64"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.11</span></span>.   For all constants \(a,c\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{20}\)</span>


<!--



                                                                                                        P (X = c) = 1 =⇒ E(X) = c                                  (1.21)   --><a id="eq:expect1"></a><!--
                                                                                                         Y = aX + c =⇒ E(Y ) = aE(X) + c                           (1.22)   --><a id="eq:expect2"></a><!--



-->


<p>


\begin{align}
\label {eq:expect1}P(X=c)=1&amp;\implies E(X)=c\\ Y=aX+c&amp;\implies E(Y)=aE(X)+c\label {eq:expect2}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-65"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> By definition,
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:expect1">1.21</a>)</span>
</p>
<p>
\[ E(X)=\sum _i x_ip(x_i)=cP(X=c)=c \]
</p>
</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:expect2">1.22</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{22}\)</span>


<!--

                                                                                                              X
                                                                                                    E(Y ) =       (axi + b)p(xi )
                                                                                                              i
                                                                                                              X                       X
                                                                                                         =a           xi p(xi ) + b       p(xi )
                                                                                                                  i                   i

                                                                                                         = aE(X) + b


-->


<p>


\begin{align*}
E(Y)&amp;=\sum _i (ax_i+b)p(x_i)\\ &amp;=a\sum _ix_ip(x_i)+b\sum _ip(x_i)\\ &amp;=aE(X)+b\qedhere
\end{align*}


</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-66"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.12</span></span>.   If a random variable \(X\) has a symmetric probability mass function and \(E(X)\) exists, then \(E(X)\) is the central
value.
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-67"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Suppose the pmf is symmetric about zero, so \(X=\{0,\pm x_1,\pm x_2,\dots \}\) with respective probabilities \(p_0,p_1,\dots \). Then
</p>

<p>
\[ E(X)=0p_0+(x_1p_1-x_1p_1)+(x_2p_2-x_2p_2)+\dots =0 \]
</p>

<p>
If \(X\) is symmetric about some other value \(\mu \), then define \(Y=X-\mu \) to get a pmf symmetric about zero, so \(E(Y)=0\). Then by equation <span class="textup">(<a href="MATH0057.html#eq:expect1">1.21</a>)</span>,
</p>

<p>
\[ E(X)=E(Y)+\mu =\mu \qedhere \]
</p>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<h6 id="autosec-68"><span class="sectionnumber">1.2.3&#x2003;</span>Variance</h6>
<a id="MATH0057-autopage-68"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-69"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Variance)</span>.   When it exists, the \(r^\text {th}\) moment of \(X\) about \(\alpha \) is
</p>

<p>
\[ E\big ((X-\alpha )^r\big ) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-70"></a>

<p>
<span class="amsthmnamedefinition">Definition</span>.   For a random variable \(X\) with mean \(E(X)=\mu \), the variance of \(X\) is
</p>

<p>
\[ \Var (X)=E\big ((X-\alpha )^2\big ) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-71"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   \(E(X)\) is the first moment about zero, and \(\Var (X)\) is the second moment about \(\mu =E(X)\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-72"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.13</span></span>.   For a random variable \(X\) with \(\mu =E(X)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{22}\)</span>


<!--



                                                                                                         Var(X) = E(X 2 ) − µ2                                                          (1.23)   --><a id="eq:var1"></a><!--
                                                                                                                                 
                                                                                                         Var(X) = E X(X − 1) − µ(µ − 1)                                                 (1.24)   --><a id="eq:var2"></a><!--



-->


<p>


\begin{align}
\label {eq:var1}\Var (X)&amp;=E(X^2)-\mu ^2\\ \label {eq:var2}\Var (X)&amp;=E\big (X(X-1)\big )-\mu (\mu -1)
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-73"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Using equation <span class="textup">(<a href="MATH0057.html#eq:expect1">1.21</a>)</span> gives
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:var1">1.23</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{24}\)</span>


<!--

                                                                                                                      
                                                                                                        E (X − α)2 = E(X 2 − 2µX + µ2 )

                                                                                                                          = E(X 2 ) − 2µE(X) + µ2
                                                                                                                          = E(X 2 ) − 2µµ + µ2
                                                                                                                          = E(X 2 ) − µ2


-->


<p>


\begin{align*}
E\big ((X-\alpha )^2\big )&amp;=E(X^2-2\mu X+\mu ^2)\\ &amp;=E(X^2)-2\mu E(X)+\mu ^2\\ &amp;=E(X^2)-2\mu \mu +\mu ^2\\ &amp;=E(X^2)-\mu ^2
\end{align*}


</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:var2">1.24</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{24}\)</span>


<!--

                                                                                                               
                                                                                                    E (X − α)2 = E(X 2 − X + X(1 − 2µ) + µ2 )

                                                                                                                   = E(X 2 − X) + (1 − 2µ)E(X) + µ2
                                                                                                                                   
                                                                                                                   = E X(X − 1) + (1 − 2µ)µ + µ2
                                                                                                                                   
                                                                                                                   = E X(X − 1) − µ(µ − 1)


-->


<p>


\begin{align*}
E\big ((X-\alpha )^2\big )&amp;=E(X^2-X+X(1-2\mu )+\mu ^2)\\ &amp;=E(X^2-X)+(1-2\mu )E(X)+\mu ^2\\ &amp;=E\big (X(X-1)\big )+(1-2\mu )\mu +\mu ^2\\ &amp;=E\big (X(X-1)\big )-\mu (\mu -1)\qedhere
\end{align*}


</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-74"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.14</span></span>.   For a random variable \(X\) and all constants \(a,c\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{24}\)</span>


<!--



Where it exists,                                                                                                                                           (1.25)                                    Var(X) ≥ 0    --><a id="eq:var3"></a><!--
                                                                                                                                                           (1.26)                                                --><a
                                                                                                                                                                                                Var(X) = 0 ⇐⇒ P (X      id="eq:var4"></a><!--
                                                                                                                                                                                                                   = c) = 1
                                                                                                                                                           (1.27)                                                  ) = aid="eq:var5"></a><!--
                                                                                                                                                                                                                --><a
                                                                                                                                                                                               Y = aX + c =⇒ Var(Y       2
                                                                                                                                                                                                                           Var(X)



-->


<p>


\begin{flalign}
\label {eq:var3}\text {Where it exists,}&amp;&amp;\Var (X)\!\!\!\!\!&amp;\,\,\,\,\,\ge 0\\ \label {eq:var4}&amp;&amp;\Var (X)=0&amp;\iff P(X=c)=1\\ \label {eq:var5}&amp;&amp;Y=aX+c&amp;\implies \Var (Y)=a^2\Var (X)\qquad \quad \phantom
{.}
\end{flalign}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-75"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:var3">1.25</a>)</span> Variance is a sum of non-negative terms
</p>
<p>
\[ \Var (X)=E(X-\mu )^2=\sum _i (x_i-\mu )p(x_i) \]
</p>
</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:var4">1.26</a>)</span> By equation <span class="textup">(<a href="MATH0057.html#eq:expect1">1.21</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{27}\)</span>


<!--


                                                                                                                       P (X = c) = 1 =⇒ E(X) = c

so by the definition of varience,

                                                                                                                                    =⇒ Var(X) = 0


-->


<p>


\begin{align*}
P(X=c)=1&amp;\implies E(X)=c \intertext {so by the definition of varience,} &amp;\implies \Var (X)=0
\end{align*}


</p>
<p>
Now suppose \(X\) is not constant. Then \(X\) takes at least two values with non-zero probability. Therefore at least one term in
</p>
<p>
\[ \Var (X)=E(X-\mu )^2=\sum _i (x_i-\mu )p(x_i) \]
</p>
<p>
must not be zero, so \(\Var (X)&gt;1\). Hence in this case, \(\Var (X)\ne 0\).
</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:var5">1.27</a>)</span> By equation <span class="textup">(<a href="MATH0057.html#eq:expect2">1.22</a>)</span>,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{27}\)</span>


<!--

                                                                                                                                          2
                                                                                                                    Var(Y ) = E Y − E(Y )
                                                                                                                                                     2
                                                                                                                            = E aX + c − aE(X) − c
                                                                                                                                               2
                                                                                                                            = a2 E X − E(X)

                                                                                                                            = a2 Var(X)


-->


<p>


\begin{align*}
\Var (Y)&amp;=E\big (Y-E(Y)\big )^2\\ &amp;=E\big (aX+c-aE(X)-c\big )^2\\ &amp;=a^2E\big (X-E(X)\big )^2\\ &amp;=a^2\Var (X)\qedhere
\end{align*}


</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-76"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Standard definition)</span>.   Where it exists, the standard deviation of \(X\) is
</p>

<p>
\[ \sd (X)=\sqrt {\Var (X)} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-77"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Unlike variance, standard deviation is measured in the same units of \(X\), so has a direct interpretation.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-78"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.   Expectation, variance and standard deviation may be notated as \(\mu ,\sigma ^2,\sigma \) respectively, or if necessary, \(\mu _X,\mu _Y,\sigma ^2_X,\dots \).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-79"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Expectation may be interpreted as a measure of location, and varience and standard deviation as measures of spread or dispersion.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-80"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Coefficient of variation)</span>.   The coefficient of variation of a random variable \(X\) is the dimensionless ratio
</p>

<p>
\[ \frac \sigma \mu \]
</p>

<p>


</p>

</li>

</ul>

</div>
<h6 id="autosec-81"><span class="sectionnumber">1.2.4&#x2003;</span>Probability generating function</h6>
<a id="MATH0057-autopage-81"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-82"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (PGF)</span>.   For a random variable \(X\) taking values \(0,1,2,\dots \), the probability generation function (pgf) of \(X\) is the power series
</p>

<p>
\[ \Pi _X(z)=E(z^X)=\sum _{k=0}^\infty z^kp(k) \]
</p>

<p>
for all values of \(z\) where the expectation is defined.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-83"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.15</span></span>.            For a random variable \(X\) with pgf \(\Pi (z)\), where the expectations exist, the \(n^\text {th}\) factorial moment
of \(X\) is
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{27}\)</span>


<!--



                                                                                                                Π(1) = 1                                                                                 (1.28)                        --><a id="eq:pgf1"></a><!--
                                                                                                                 ′
                                                                                                               Π (1) = E(X)                                                                              (1.29)                        --><a id="eq:pgf2"></a><!--
                                                                                                                ′′
                                                                                                                                           
                                                                                                               Π (1) = E X(X − 1)                                                                        (1.30)                        --><a id="eq:pgf3"></a><!--
                                                                                                                                  X!
                                                                                                                                          
                                                                                                             Π(n) (1) = E                                                                                (1.31)                        --><a id="eq:pgf4"></a><!--
                                                                                                                                (X − n)!


-->


<p>


\begin{align}
\label {eq:pgf1}\Pi (1)&amp;=1\\ \label {eq:pgf2}\Pi &apos;(1)&amp;=E(X)\\ \label {eq:pgf3}\Pi &apos;&apos;(1)&amp;=E\big (X(X-1)\big )\\ \label {eq:pgf4}\Pi ^{(n)}(1)&amp;=E\left (\frac {X!}{(X-n)!}\right )
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-84"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof by induction.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:pgf1">1.28</a>)</span>
</p>
<p>
\[ \Pi (1)=E(1)=1 \]
</p>
</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:pgf4">1.31</a>)</span> Obvious, by induction on the previous method.
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                                     dΠ(n) (z)
                                                                                                      Π(n+1) (z) =
                                                                                                                       dz
                                                                                                                            ∞
                                                                                                                                                    !
                                                                                                                  d         X        k!
                                                                                                                =                          z k−n p(k)
                                                                                                                  dz              (k − n)!
                                                                                                                            k=n
                                                                                                                     ∞
                                                                                                                     X        k!
                                                                                                                =                   (k − n)z k−(n+1) p(k)
                                                                                                                           (k − n)!
                                                                                                                     k=n
                                                                                                                       ∞
                                                                                                                     X               k!
                                                                                                                =                            z k−(n+1) p(k)
                                                                                                                              k − (n + 1) !
                                                                                                                     k=n+1


-->


<p>


\begin{align*}
\Pi ^{(n+1)}(z)&amp;=\dd {\Pi ^{(n)}(z)}{z}\\ &amp;=\dd {}z\left (\sum _{k=n}^\infty \frac {k!}{(k-n)!}z^{k-n}p(k)\right )\\ &amp;=\sum _{k=n}^\infty \frac {k!}{(k-n)!} (k-n)z^{k-(n+1)}p(k)\\ &amp;=\sum _{k=n+1}^\infty \frac {k!}{\big
(k-(n+1)\big )!} z^{k-(n+1)}p(k)
\end{align*}
so
</p>
<p>
\[ \Pi ^{(n)}(1)=\sum _{k=n}^\infty \frac {k!}{(k-n)!}p(k)=\sum _{k=0}^\infty \frac {k!}{(k-n)!}p(k)=E\left (\frac {X!}{(X-n)!}\right ) \qedhere \]
</p>
<p>


</p>
</li>
</ul>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<h5 id="autosec-85"><span class="sectionnumber">1.3&#x2003;</span>Standard discrete distributions</h5>
<a id="MATH0057-autopage-85"></a>
<h6 id="autosec-86"><span class="sectionnumber">1.3.1&#x2003;</span>Choosing a distribution</h6>
<a id="MATH0057-autopage-86"></a>
<p>
<span class="paragraph" id="autosec-87"><a href="MATH0057.html#sec:ber">Bernoulli</a></span>
<a id="MATH0057-autopage-87"></a>
An experiment with two outcomes that form a partition of the sample space
</p>

<p>
\[ X:\Omega \to \{0,1\} \]
</p>

<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                         “success” = {ω ∈ Ω : X(ω) = 1}
                                                                                                         “faliure” = {ω ∈ Ω : X(ω) = 0}


-->


<p>

\begin{align*}
\text {&grave;&grave;success&apos;&apos;}&amp;=\{\omega \in \Omega :X(\omega )=1\}\\ \text {&grave;&grave;faliure&apos;&apos;}&amp;=\{\omega \in \Omega :X(\omega )=0\}
\end{align*}
may be called a binary experiment, or Bernoulli trial. \(X\) has a Bernoulli distribution
</p>
<p>
<span class="paragraph" id="autosec-88"><a href="MATH0057.html#sec:bio">Binomial</a></span>
<a id="MATH0057-autopage-88"></a>
The number of successes of \(n\in \N 1\) independent Bernoulli trials, each with the same probability of success.
</p>
<p>
<span class="paragraph" id="autosec-89"><a href="MATH0057.html#sec:geo">Geometric</a></span>
<a id="MATH0057-autopage-89"></a>
The number of independent Bernoulli trials, each with the same probability of success, until a success is observed.
</p>
<p>
<span class="paragraph" id="autosec-90"><a href="MATH0057.html#sec:bin">Negative Binomial</a></span>
<a id="MATH0057-autopage-90"></a>
The number of independent Bernoulli trials, each with the same probability of success, until a \(r\in \N 1\) successes are observed.
</p>
<p>
<span class="paragraph" id="autosec-91"><a href="MATH0057.html#sec:hyp">Hypergeometric</a></span>
<a id="MATH0057-autopage-91"></a>
The number of objects with an attribute, when \(n\) objects are drawn from a population of \(N\), with \(M\) having that attribute.
</p>
<p>
<span class="paragraph" id="autosec-92"><a href="MATH0057.html#sec:poi">Poisson</a></span>
<a id="MATH0057-autopage-92"></a>
The number of instantaneous occurrences within an interval.
</p>
<h6 id="autosec-93"><span class="sectionnumber">1.3.2&#x2003;</span>Bernoulli</h6>
<a id="MATH0057-autopage-93"></a>


<a id="sec:ber"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-94"></a>

<p>
<span class="amsthmnamedefinition">Rule</span><span class="amsthmnumberdefinition"> <span class="textup">1.16</span></span>.        For \(P(\text {success})=p\) and,
</p>

<p>
\[ X=\begin {cases*} 1&amp;if success\\ 0&amp;if faliure \end {cases*} \]
</p>

<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                   E(X) = 1p + 0(1 − p) = p
                                                                                                  E(X 2 ) = 12 p + 02 (1 − p) = p
                                                                                                                             2
                                                                                                  Var(X) = E(X 2 ) − E(X)         = p − p2 = p(1 − p)



-->


<p>


\begin{align*}
E(X)&amp;=1p+0(1-p)=p\\ E(X^2)&amp;=1^2p+0^2(1-p)=p\\ \Var (X)&amp;=E(X^2)-\big (E(X)\big )^2=p-p^2=p(1-p)
\end{align*}


</p>

</li>

</ul>

</div>
<h6 id="autosec-95"><span class="sectionnumber">1.3.3&#x2003;</span>Binomial</h6>
<a id="MATH0057-autopage-95"></a>


<a id="sec:bio"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-96"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.   The number of successes \(X\) of \(n\) independent Bernoulli trials, each with the same probability of success \(p\), follows a binomial distribution with parameters \(n\) and \(p\), written
</p>

<p>
\[ X\sim \Bin (n,p) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-97"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.17</span></span>.   \(X\sim \Bin (n,p)\) has pmf
</p>

<p>
\[ P(X=k)=\binom {n}{k}p^k(1-p)^{n-k} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-98"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof omitted.</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-99"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.18</span></span>.   For \(X\sim \Bin (n,p)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                 E(X) = np
                                                                                                                Var(X) = np(1 − p)



-->


<p>


\begin{align*}
E(X)&amp;=np\\ \Var (X)&amp;=np(1-p)
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-100"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Let each independent Bernoulli random variable be \(Y_1,\dots ,Y_n\) such that
</p>

<p>
\[ X=\sum _{i=1}^n Y_i \]
</p>

<p>
By proposition <a href="MATH0057.html#thm:inexp">1.46</a>,
</p>

<p>
\[ E(X)=E\left (\sum _{i=1}^n Y_i\right )=\sum _{i=1}^n E(Y_i)=\sum _{i=1}^n p=np \]
</p>

<p>
and by proposition <a href="MATH0057.html#thm:invar">1.48</a>,
</p>

<p>
\[ \Var (X)=\Var \left (\sum _{i=1}^n Y_i\right )=\sum _{i=1}^n \Var (Y_i)=\sum _{i=1}^n p(1-p)=np(1-p)\qedhere \]
</p>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<h6 id="autosec-101"><span class="sectionnumber">1.3.4&#x2003;</span>Geometric</h6>
<a id="MATH0057-autopage-101"></a>


<a id="sec:geo"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-102"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.   The number \(X\) of independent Bernoulli trials, each with the same probability of success \(p\), until a success is observed, follows a geometric distribution with parameters \(p\), written
</p>

<p>
\[ X\sim \Geo (p) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-103"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.19</span></span>.   \(X\sim \Geo (p)\) has pmf
</p>

<p>
\[ P(X=k)=(1-p)^{k-1}p \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-104"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> The derivation comes from that \(k-1\) trials must fail before the \(k^\text {th}\) succeeds. Now check that this is a pmf. Clearly it is always non-negative. By the sum of a geometric series,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                 ∞                   ∞                        ∞
                                                                                                 X                   X                        X                           1
                                                                                                       P (X = k) =         (1 − p)k−1 p = p         (1 − p)k−1 = p               =1
                                                                                                                                                                     1 − (1 − p)
                                                                                                 k=1                 k=1                      k=1



-->


<p>


\begin{align*}
\sum _{k=1}^\infty P(X=k)&amp;=\sum _{k=1}^\infty (1-p)^{k-1}p =p\sum _{k=1}^\infty (1-p)^{k-1} =p\frac 1{1-(1-p)} =1\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-105"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.20</span></span>.   For \(X\sim \Geo (p)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                          1
                                                                                                                   E(X) =
                                                                                                                          p
                                                                                                                          1−p
                                                                                                                 Var(X) =
                                                                                                                            p2


-->


<p>


\begin{align*}
E(X)&amp;=\frac 1p\\ \Var (X)&amp;=\frac {1-p}{p^2}
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-106"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> The pgf for \(X\) is
</p>

<p>
\[ \Pi (z)=E(z^X)=\sum _{k=1}^\infty z^kp(1-p)^{k-1}=pz\sum _{k=1}^\infty \big (z(1-p)\big )^{k-1}=\frac {pz}{1-(1-p)z} \]
</p>

<p>
for \(\abs {(1-p)z}\le 1\). Then
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                     p                      pz
                                                                                                   Π′ (z) =                 + (1 − p)
                                                                                                               1 − (1 − p)z           (1 − (1 − p)z)2
                                                                                                               p(1 − (1 − p)z) + (1 − p)pz
                                                                                                          =
                                                                                                                     (1 − (1 − p)z)2
                                                                                                                      p
                                                                                                          =
                                                                                                               (1 − (1 − p)z)2
                                                                                                                  2p(1 − p)
                                                                                                   Π′′ (z) =
                                                                                                               (1 − (1 − p)z)3


-->


<p>


\begin{align*}
\Pi &apos;(z)&amp;=\frac {p}{1-(1-p)z}+(1-p)\frac {pz}{\left (1-(1-p)z\right )^2}\\ &amp;=\frac {p(1-(1-p)z)+(1-p)pz}{\left (1-(1-p)z\right )^2}\\ &amp;=\frac {p}{\left (1-(1-p)z\right )^2}\\ \Pi &apos;&apos;(z)&amp;=\frac {2p(1-p)}{\left
(1-(1-p)z\right )^3}
\end{align*}
Hence
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                             1
                                                                                                     E(X) = Π′ (1) =
                                                                                                                             p
                                                                                                                                                  
                                                                                                    Var(X) = E X(X − 1) − E(X) E(X) − 1
                                                                                                                             1
                                                                                                                             1
                                                                                                                                        
                                                                                                               = Π′′ (1) −     −1
                                                                                                                             p
                                                                                                                             p
                                                                                                                 2(1 − p)   11−p    1−p
                                                                                                               =          −       =
                                                                                                                    p2      p p      p2


-->


<p>


\begin{align*}
E(X)&amp;=\Pi &apos;(1)=\frac 1p\\ \Var (X)&amp;=E\big (X(X-1)\big )-E(X)\big (E(X)-1\big )\\ &amp;=\Pi &apos;&apos;(1)-\frac 1p\left (\frac 1p-1\right )\\ &amp;=\frac {2(1-p)}{p^2}-\frac 1p\frac {1-p}{p}=\frac {1-p}{p^2}\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-107"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   If asked to calculate the probability \(P(X&gt;c)\), don’t use the geometric distribution, just consider that there are \(c\) fails first, so the probability is \((1-p)^c\).
</p>

</li>

</ul>

</div>
<h6 id="autosec-108"><span class="sectionnumber">1.3.5&#x2003;</span>Negative Binomial</h6>
<a id="MATH0057-autopage-108"></a>


<a id="sec:bin"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-109"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.   The number \(X\) of independent Bernoulli trials, each with the same probability of success \(p\), up to and including the \(r^\text {th}\) success, follows a negative binomial distribution with parameters \(r\)
and \(p\), written
</p>

<p>
\[ X\sim \NB (r,p) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-110"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.21</span></span>.   \(X\sim \NB (r,p)\) has pmf
</p>

<p>
\[ P(X=k)=\binom {k-1}{r-1}p^{r-1}(1-p)^{k-r}\times p=\binom {k-1}{r-1}p^r(1-p)^{k-r} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-111"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> This formula may be derived from that there must be \(r-1\) successes in the first \(k-1\) throws, then one success. Now check that this is a pmf. Clearly it is always non-negative. By negative binomial expansion,
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                               ∞                   ∞
                                                                                                               X                   X k − 1
                                                                                                                     P (X = k) =                   pr (1 − p)k−r
                                                                                                                                        r−1
                                                                                                               k=r                 k=r
                                                                                                                                      ∞
                                                                                                                                     X k − 1
                                                                                                                                    r
                                                                                                                              =p                     (1 − p)k−r
                                                                                                                                           r−1
                                                                                                                                     k=r
                                                                                                                                     ∞
                                                                                                                                   r
                                                                                                                                     X r + k
                                                                                                                              =p                     (1 − p)k
                                                                                                                                               r
                                                                                                                                     k=0
                                                                                                                                          pr
                                                                                                                              =                    r = 1
                                                                                                                                   (1 − (1 − p)


-->


<p>


\begin{align*}
\sum _{k=r}^\infty P(X=k)&amp;=\sum _{k=r}^\infty \binom {k-1}{r-1}p^r(1-p)^{k-r}\\ &amp;=p^r\sum _{k=r}^\infty \binom {k-1}{r-1}(1-p)^{k-r}\\ &amp;=p^r\sum _{k=0}^\infty \binom {r+k}{r}(1-p)^k\\ &amp;=\frac {p^r}{(1-(1-p)\big
)^{r}}=1\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-112"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.22</span></span>.   For \(X\sim \NB (r,p)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                         r
                                                                                                                  E(X) =
                                                                                                                         p
                                                                                                                         r(1 − p)
                                                                                                                Var(X) =
                                                                                                                            p2


-->


<p>


\begin{align*}
E(X)&amp;=\frac {r}p\\ \Var (X)&amp;=\frac {r(1-p)}{p^2}
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-113"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Write \(X\) as the sum of \(r\) independent geometric random variables,
</p>

<p>
\[ X=Y_1+\cdots +Y_r \]
</p>

<p>
each having parameter \(p\). By proposition <a href="MATH0057.html#thm:inexp">1.46</a>,
</p>

<p>
\[ E(X)=E\left (\sum _{i=1}^r Y_i\right )=\sum _{i=1}^r E(Y_i)=\sum _{i=1}^r \frac {1}p=n\frac {r}p \]
</p>

<p>
and by proposition <a href="MATH0057.html#thm:invar">1.48</a>,
</p>

<p>
\[ \Var (X)=\Var \left (\sum _{i=1}^r Y_i\right )=\sum _{i=1}^r \Var (Y_i)=\sum _{i=1}^r \frac {1-p}{p^2}=\frac {r(1-p)}{p^2}\qedhere \]
</p>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-114"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   As with the geometric distribution, if asked to calculate the probability \(P(X&gt;c)\), don’t use the negative binomial distribution, use the binomial distribution on the first \(c\) trials.
</p>

</li>

</ul>

</div>
<h6 id="autosec-115"><span class="sectionnumber">1.3.6&#x2003;</span>Hypergeometric</h6>
<a id="MATH0057-autopage-115"></a>


<a id="sec:hyp"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-116"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.      Consider a population of finite size \(N\), with \(M\) individuals of interest, and \(n\) objects are sampled at random. If objects are sampled with replacement, then the number of objects \(X\) out of \(n\) of
interest is
</p>

<p>
\[ X\sim \Bin (n,\tfrac {M}N) \]
</p>

<p>
Otherwise, if objects are sampled without replacement,
</p>

<p>
\[ X\sim \Hyp (n,M,N) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-117"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.23</span></span>.           \(X\sim \Hyp (n,M,N)\) has pmf
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                                                   N −M
                                                                                                                           M
                                                                                                                                         
                                                                                                                           k        n−k
                                                                                                             P (X = k) =           N
                                                                                                                                    
                                                                                                                                   n



-->


<p>


\begin{align*}
P(X=k)&amp;=\frac {\binom {M}{k}\binom {N-M}{n-k}}{\binom {N}n}
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-118"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> For \(X=k\), \(k\) objects of interest out of \(M\) must be selected and \(n-k\) objects out of \(N-M\) must be selected. Hence the chance of this happening when \(n\) objects are chosen from \(N\) is given by the formula.
Now check that this is a pmf. Clearly it is always non-negative.
</p>

<p>
Use the binomial theorem.
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                (1 + x)N = (1 + x)M (1 + x)N −M
                                                                                                                                M
                                                                                                                                                  !   N −M
                                                                                                                                                                       !
                                                                                                                                X M                  X N − M
                                                                                                                          =                  xr                   xr
                                                                                                                                         r                   r
                                                                                                                                r=0                   r=0

The \(x_n\) coefficient of this is
                                                                                                                               n
                                                                                                                     N X M N − M
                                                                                                                       =
                                                                                                                     n    k   n−k
                                                                                                                              k=0

If \(n&gt;N-M\), the first few terms will be zero, and if \(M&lt;n\), then the terms for \(k&gt;M\) will be zero. Hence
                                                                                                                              min(n,M )
                                                                                                                     N         X            M N − M
                                                                                                                        =
                                                                                                                     n                       k   n−k
                                                                                                                                   k=0



-->


<p>


\begin{align*}
(1+x)^N&amp;=(1+x)^M(1+x)^{N-M}\\ &amp;=\left (\sum _{r=0}^M\binom {M}rx^r\right )\left (\sum _{r=0}^{N-M}\binom {N-M}rx^r\right ) \intertext {The $x_n$ coefficient of this is} \binom {N}n&amp;=\sum _{k=0}^{n}\binom {M}k\binom {N-M}{n-k}
\intertext {If $n&gt;N-M$, the first few terms will be zero, and if $M&lt;n$, then the terms for $k&gt;M$ will be zero.                           Hence} \binom {N}n&amp;=\sum _{k=0}^{\min (n,M)}\binom {M}k\binom {N-M}{n-k}
\end{align*}
and so the sum of probabilities is 1. <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-119"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.24</span></span>.   For \(X\sim \Hyp (n,M,n)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                   nM
                                                                                                            E(X) =
                                                                                                                    N
                                                                                                                   nM (N − M )(N − n)
                                                                                                          Var(X) =
                                                                                                                       N 2 (N − 1)


-->


<p>


\begin{align*}
E(X)&amp;=\frac {nM}N\\ \Var (X)&amp;=\frac {nM(N-M)(N-n)}{N^2(N-1)}
\end{align*}


</p>

</li>

</ul>

</div>
<h6 id="autosec-120"><span class="sectionnumber">1.3.7&#x2003;</span>Poisson</h6>
<a id="MATH0057-autopage-120"></a>


<a id="sec:poi"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-121"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The lecturer uses language of accidents in time intervals, but I generalised this to incidents in measures.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-122"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Poisson process)</span>.            For some constant rate of process \(\lambda \), a Poisson process satisfies
</p>
<ul style="list-style-type:none">


<li>
<p>
1. The probability of one incident during some measure \(\delta \) is \(\delta \lambda +o(\lambda )\).
</p>


</li>
<li>
<p>
2. The probability of no incidents during the same measure is \(1-\lambda \delta + o(\lambda )\).
</p>


</li>
<li>
<p>
3. If they don’t overlap, the number of incidents in any measure is independent of any other.
</p>
</li>
</ul>

<p>
The first two assumption ensure that for small enough \(\delta \), the probability of an incident during a measure is approximately zero, thus two coincident occurrences are prohibited.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-123"></a>

<p>
<span class="amsthmnamedefinition">Notation</span>.   Notate the distribution of the number of accidents in a Poisson process \(X\), during an measure, as the Poisson distribution
</p>

<p>
\[ X\sim \Poi (\lambda ) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-124"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.25</span></span>.    For \(\lambda =\mu \), \(X\sim \Poi (\lambda )\) has pmf
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                              λk e−λ
                                                                                                                P (X = k) =
                                                                                                                                k!


-->


<p>


\begin{align*}
P(X=k)&amp;=\frac {\lambda ^ke^{-\lambda }}{k!}
\end{align*}
and
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                    E(X) = µ
                                                                                                                  Var(X) = mu



-->


<p>


\begin{align*}
E(X)&amp;=\mu \\ \Var (X)&amp;=mu
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-125"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.       Split the measure \([0,t]\) into \(n\) intervals of length \(\delta =\frac {t}n\), and set up a Bernoulli trial in each interval, where success means an incident occurred. Then taking the limit as \(n\) gets
bigger, and \(\lambda \) gets smaller,
</p>

<p>
\[ X(t)\sim Po(\lambda )\approx \Bin (n,\lambda \delta )=\Bin (n,\tfrac {\lambda t}n) \]
</p>

<p>
Hence
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                                                       λt
                                                                                                                          E X(t) = n       = λt
                                                                                                                                        n
                                                                                                                                  
                                                                                                                          E X(1) = λ



-->


<p>


\begin{align*}
E\big (X(t)\big )&amp;=n\frac {\lambda t}{n}=\lambda t\\ E\big (X(1)\big )&amp;=\lambda
\end{align*}
Thus \(\lambda =\mu \) is the mean number of incidents in a measure of 1 unit.
</p>

</li>

</ul>

</div>
<h5 id="autosec-126"><span class="sectionnumber">1.4&#x2003;</span>Continuous random variables</h5>
<a id="MATH0057-autopage-126"></a>
<h6 id="autosec-127"><span class="sectionnumber">1.4.1&#x2003;</span>Probability density functions</h6>
<a id="MATH0057-autopage-127"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-128"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Continuous random variable)</span>.   A random variable is a function from a sample space \(\Omega \) to \(\R \). For associated event space \(\mathcal {F}\),
probability function \(P\) and random variable \(X:\Omega \to \R \), for any real \(a,b\in \R \),
</p>

<p>
\[ a\le X\le b=\{\omega :a\le X(\omega )\le b\}\in \mathcal {F} \]
</p>

<p>
In particular, the cumulative distribution function may be written
</p>

<p>
\[ F_X(x)=P(X\le x) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-129"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Probability density function)</span>.                  If \(F_X\) is continuous and differentiable, with derivative \(f_X\), then \(X\) is a continuous random variable. \(f_X\) is the
probability density function of \(X\). If unambiguous, the subscript \(X\) may be omitted.
</p>

<p>
Differentiation and integration are inverse operations, and \(F(-\infty )=0\), so
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--


                                                                                                                                        Z   x
                                                                                                                              F (x) =           f (u) du
                                                                                                                                        −∞
                                                                                                                                                 Z    b
                                                                                                             P (a < X ≤ b) = F (b) − F (a) =              f (x) dx   for   a≤b
                                                                                                                                                  a



-->


<p>


\begin{gather*}
F(x)=\int _{-\infty }^x f(u)\;\d u\\ P(a&lt;X\le b)=F(b)-F(a)=\int _a^b f(x)\;\d x\quad \text {for}\quad a\le b
\end{gather*}
This gives the interpretation that the probability that \(X\) lies in an interval \((a,b]\) is given by the area under the graph of the pdf between \(a\) and \(b\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-130"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.
</p>

<p>
\[ F(\infty )=P(X\le \infty )=\int _\R f(x)\;\d x=1 \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-131"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.26</span></span>.     The conditions required for a function \(f\) to be a pdf are
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{31}\)</span>


<!--



                                                                                                                 ∀x ∈ R     f (x) ≥ 0                                                                  (1.32)   --><a id="eq:pdf1"></a><!--
                                                                                                                 Z   ∞
                                                                                                                         f (x) dx = 1                                                                  (1.33)   --><a id="eq:pdf2"></a><!--
                                                                                                                  −∞



-->


<p>


\begin{gather}
\label {eq:pdf1}\forall x\in \R \quad f(x)\ge 0\\ \label {eq:pdf2}\int _{-\infty }^\infty f(x)\;\d x=1
\end{gather}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-132"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   A continuous random variable may be described completely by knowing either \(F\) or \(f\), since one may be obtained from the other.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-133"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   If \(X\) is continuous, then \(F\) is continuous, so
</p>

<p>
\[ P(X=x)=\lim _{\varepsilon \to 0}P(x-\varepsilon &lt;X\le x)=\lim _{\varepsilon \to 0} F(x)-F(x-\varepsilon )=0 \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-134"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   \(F\) may be neither discontinuous on a countable set nor continuous and non-differentiable. This may be harder to deal with, but these situation are unlikely to arise. As an exception, \(F\) may be a mix of the
two, which is straightforward to handle.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-135"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Median)</span>.   The median satisfies \(F(x)=\frac 12\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-136"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Mode)</span>.   The mode of \(X\) is the value of \(x\) for which \(f(x)\) is a maximum.
</p>

</li>

</ul>

</div>
<h6 id="autosec-137"><span class="sectionnumber">1.4.2&#x2003;</span>Expectation and varience</h6>
<a id="MATH0057-autopage-137"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-138"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.         From the definition of differentiation,
</p>

<p>
\[ f(x)=\lim _{\delta x\to 0}\frac {F(x+\delta x)-F(x)}{\delta x}=\lim _{\delta x\to 0}\frac {P(x&lt;X&lt;x+\delta x)}{\delta x} \]
</p>

<p>
Hence \(f(x)\) must be non-negative everywhere. Providing \(\delta x\) is small,
</p>

<p>
\[ P(x&lt;X&lt;x+\delta x)\approx f(x)\:\delta x \]
</p>

<p>
so a continuous random variable may be approximated by a discrete one which takes values on a finely spaced grid of points, separated by intervals of length \(\delta x\).Then the probability associated with an interval starting at point \(x\) would be \(f(x)\:\delta
x\).
</p>

<p>
The expectation of such a discrete random variable is the sum over every \(x\) value in the grid of
</p>

<p>
\[ \sum xf(x)\:\delta x \]
</p>

<p>
As \(\delta x\to 0\), this sum tends to an integral, motivating the following definitions. Generally, continuous random variable formulae look similarly to discrete random variable formulae.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-139"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Expectation)</span>.   For continuous random variable \(X\) with density \(f\), the expected value of \(X\) is
</p>

<p>
\[ E(X)=\int _{-\infty }^\infty xf(x)\;\d x \]
</p>

<p>
if the integral converges absolutely
</p>

<p>
\[ \int _{-\infty }^\infty \abs {x}f(x)\;\d x&lt;\infty \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-140"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Variance)</span>.   For continuous random variable \(X\) with density \(f\) and expectation \(\mu =E(X)\),
</p>

<p>
\[ \Var (X)=E(X-\mu )^2=\int _{-\infty }^\infty (x-\mu )^2 f(x)\;\d x \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-141"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Expectation and variance for continues random variable inherit the same properties as for the discrete case.
</p>

</li>

</ul>

</div>
<h6 id="autosec-142"><span class="sectionnumber">1.4.3&#x2003;</span>Moment generating function</h6>
<a id="MATH0057-autopage-142"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-143"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Moment generating function)</span>.   The moment generating function (mgf) of a continuous random variable \(X\) is
</p>

<p>
\[ M_X(t)=E(e^{tX}) \]
</p>

<p>
for those values of \(t\) where the expectation exists.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-144"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Note that
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{33}\)</span>


<!--


                                                                                                                                                 
                                                                                                                                    (tX)2
                                                                                                 M (t) = E(etX ) =    1 + tX +            + ···
                                                                                                                                      2!

At \(t=0\),

                                                                                                          M (0) = E(e0X ) = E(1) = 1

Differentiating with respect to \(t\) gives
                                                                                                                                                     
                                                                                                             ′                      2 t2 X 3
                                                                                                          M (t) = E          X + tX +        + ···
                                                                                                                                        2!

                                                                                                         M ′ (0) = E(X + 0 + · · · ) = E(X)
                                                                                                                                                     
                                                                                                            ′′                2       t2 X 4
                                                                                                                                        3
                                                                                                         M (t) = E           X + tX +        + ···
                                                                                                                                        2!

                                                                                                         M ′′ (0) = E(X 2 + 0 + 0 + · · · ) = E(X 2 )

The \(k^\text {th}\) moment of \(X\) is

                                                                                                       M (k) (0) = E(X k )



-->


<p>


\begin{align*}
M(t)=E(e^{tX})&amp;=\left (1+tX+\frac {(tX)^2}{2!}+\cdots \right ) \intertext {At $t=0$,} M(0)&amp;=E(e^{0X})=E(1)=1 \intertext {Differentiating with respect to $t$ gives} M&apos;(t)&amp;=E\left (X+tX^2+\frac {t^2X^3}{2!}+\cdots \right
)\\ M&apos;(0)&amp;=E(X+0+\cdots )=E(X)\\ M&apos;&apos;(t)&amp;=E\left (X^2+tX^3+\frac {t^2X^4}{2!}+\cdots \right )\\ M&apos;&apos;(0)&amp;=E(X^2+0+0+\cdots )=E(X^2) \intertext {The $k^\text {th}$ moment of $X$ is} M^{(k)}(0)&amp;=E(X^k)
\end{align*}


</p>

</li>

</ul>

</div>
<h6 id="autosec-145"><span class="sectionnumber">1.4.4&#x2003;</span>Functions of random variables</h6>
<a id="MATH0057-autopage-145"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-146"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.27</span></span>.   For \(Y=g(X)\) for a strictly monotone function \(g\), where \(X\) has pdf \(f_X\), the pdf of \(Y\) is
</p>

<p>
\[ f_Y(y)=f_X\big (g^{-1}(y)\big )\abs {\dd {}{y}\big (g^{-1}(y)\big )}=f_X(x)\abs {\dd {y}{x}} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-147"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Suppose \(g\) is strictly increasing, that is,
</p>

<p>
\[ x_1&lt;x_2\implies g(x_1)&lt;g(x_2) \]
</p>

<p>
Then the cdf of \(Y\) is
</p>

<p>
\[ F_Y=P(Y\le y)=P\big (g(X)\le y\big )=P\big (X\le g^{-1}(y)\big )=F_X\big (g^{-1}(y)\big ) \]
</p>

<p>
Differentiating gives
</p>

<p>
\[ f_Y(y)=\dd {F_Y}y=\dd {}{y}F_X\big (g^{-1}(y)\big )=f_X\big (g^{-1}(y)\big )\dd {}{y}\big (g^{-1}(y)\big )=f_X\big (g^{-1}(y)\big )\dd {x}{y} \]
</p>

<p>
and similarly for a strictly decreasing sequence. <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-148"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   If \(g\) is not monotonic, then proceed from first principles.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-149"></a>

<p>
<span class="amsthmnamedefinition">Example</span><span class="amsthmnumberdefinition"> <span class="textup">1.4</span></span>.                   For \(Z\sim \mathcal {N}(0,1)\) and \(Y=Z^2\), what is the distribution of \(Y\)?
</p>

<p>
\(g(z)=z^2\) is not monotonic in \(\R \). For \(y&gt;0\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{33}\)</span>


<!--



                                                                                                          FY (y) = P (Y ≤ y) = P (Z 2 < y)
                                                                                                                                   1               1   
                                                                                                                = P − y− 2 ≤ Z ≤ y 2
                                                                                                                         1                   1
                                                                                                                = Φ(y 2 ) − Φ(−y 2 )
                                                                                                                       1              1 
                                                                                                                 = Φ(y 2 ) − 1 − Φ(y 2 )
                                                                                                                                      2
                                                                                                                   dFY
                                                                                                          fY (y) =
                                                                                                                    dy
                                                                                                                        1 1     1
                                                                                                                 = 2ϕ(y 2 ) y − 2
                                                                                                                           2
                                                                                                                                         1
                                                                                                                    1  −y y − 2
                                                                                                                = 2√ e 2
                                                                                                                    2π      2
                                                                                                                         12        1    y 1
                                                                                                                =   1
                                                                                                                    2
                                                                                                                                y − 2 e− 2 √
                                                                                                                                             π
                                                                                                                         12         1   y
                                                                                                                    1
                                                                                                                    2
                                                                                                                                y − 2 e− 2
                                                                                                                =
                                                                                                                               Γ( 12 )


-->


<p>


\begin{align*}
F_Y(y)&amp;=P(Y\le y)=P(Z^2&lt;y)\\ &amp;=P\big (-y^{-\frac 12}\le Z\le y^\frac 12\big )\\ &amp;=\Phi (y^\frac 12)-\Phi (-y^\frac 12)\\ &amp;=\Phi (y^\frac 12)-\big (1-\Phi (y^2\frac 12)\big )\\ f_Y(y)&amp;=\dd {F_Y}{y}\\ &amp;=2\phi
(y^\frac 12)\frac 12y^{-\frac 12}\\ &amp;=2\frac 1{\sqrt {2\pi }}e^\frac {-y}2\frac {y^{-\frac 12}}2\\ &amp;=\left (\tfrac 12\right )^\frac 12y^{-\frac 12}e^{-\frac {y}2}\frac 1{\sqrt {\pi }}\\ &amp;=\frac {\left (\tfrac 12\right )^\frac
12y^{-\frac 12}e^{-\frac {y}2}}{\Gamma (\frac 12)}
\end{align*}
This is exactly the pdf for \(\Gamma (\frac 12,\frac 12)\), hence \(Y\sim \Gamma (\frac 12,\frac 12)\).
</p>

</li>

</ul>

</div>
<h5 id="autosec-150"><span class="sectionnumber">1.5&#x2003;</span>Standard continuous distributions</h5>
<a id="MATH0057-autopage-150"></a>
<h6 id="autosec-151"><span class="sectionnumber">1.5.1&#x2003;</span>Uniform</h6>
<a id="MATH0057-autopage-151"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-152"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Uniform distribution)</span>.   A continuous random variable \(X\) has uniform (or rectangular) distribution, written \(X\sim \U (a,b)\), if it has
</p>

<p>
\[ f(x)=\begin {cases} \frac 1{b-a}&amp;a\le x\le b\\ 0&amp;\text {otherwise} \end {cases} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-153"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.28</span></span>.   This is a valid pdf
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-154"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:pdf1">1.32</a>)</span> \(f(x)\ge 0\) everywhere is trivial.
</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:pdf2">1.33</a>)</span> The integral is just the area of the rectangle, \(\frac 1{b-a}(b-a)=1\).<span class="theoremendmark">&#x25A1;</span>
</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-155"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.             For \((c,c+d)\in (a,b)\),
</p>

<p>
\[ P(c&lt;X\le c+d)=\int _c^{c+d}\frac {\d x}{b-a}=\frac 1{b-a}x\big |_c^{c+d}=\frac {d}{b-a} \]
</p>

<p>
Hence the probability \(X\) lies in an interval lies only in its length, not its location.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-156"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The cdf of \(X\) is
</p>

<p>
\[ F(x)=\int _{-\infty }^x f(u)\;\d u=\begin {cases} 0&amp;x&lt;a\\ \int _a^x \frac {\d u}{b-a}=\frac {x-a}{b-a}&amp;a\le x\le b\\ 1&amp;x\ge b \end {cases} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-157"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.29</span></span>.   The expectation and variance of \(X\sim U(a,b)\) are
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{33}\)</span>


<!--



                                                                                                                         a+b
                                                                                                                  E(X) =                                                                     (1.34)   --><a id="eq:U1"></a><!--
                                                                                                                           2
                                                                                                                         (b − a)2
                                                                                                                Var(X) =                                                                     (1.35)   --><a id="eq:U2"></a><!--
                                                                                                                            12


-->


<p>


\begin{align}
\label {eq:U1}E(X)&amp;=\frac {a+b}2\\ \label {eq:U2}\Var (X)&amp;=\frac {(b-a)^2}{12}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-158"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:U1">1.34</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{35}\)</span>


<!--

                                                                                                             Z    ∞
                                                                                                    E(X) =             xf (x) dx
                                                                                                              −∞
                                                                                                             Z b
                                                                                                                      x
                                                                                                         =               dx
                                                                                                              a
                                                                                                                     b−a
                                                                                                                x2          b
                                                                                                         =
                                                                                                             2(b − a)       a
                                                                                                           b2 − a2    (b + a)(b − a)   b+a
                                                                                                         =          =                =
                                                                                                           2(b − a)      2(b − a)       2


-->


<p>


\begin{align*}
E(X)&amp;=\int _{-\infty }^\infty xf(x)\;\d x\\ &amp;=\int _a^b \frac {x}{b-a}\;\d x\\ &amp;=\frac {x^2}{2(b-a)}\Big |_a^b\\ &amp;=\frac {b^2-a^2}{2(b-a)}=\frac {(b+a)(b-a)}{2(b-a)}=\frac {b+a}2
\end{align*}


</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:U2">1.35</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{35}\)</span>


<!--

                                                                                                                 Z     ∞
                                                                                                    Var(X) =               (x − µ)2 f (x) dx
                                                                                                                   −∞
                                                                                                                 Z b                 2
                                                                                                                                a+b         dx
                                                                                                             =             x−
                                                                                                                   a
                                                                                                                                 2         b−a
                                                                                                                   1         a+b 3 b
                                                                                                                                          
                                                                                                             =           x−
                                                                                                               3(b − a)        2      a
                                                                                                                   1                         
                                                                                                             =           (b − a)3 − (a − b)3
                                                                                                               24(b − a)
                                                                                                                  (b − a)2
                                                                                                             =
                                                                                                                     12


-->


<p>


\begin{align*}
\Var (X)&amp;=\int _{-\infty }^\infty (x-\mu )^2f(x)\;\d x\\ &amp;=\int _a^b \left (x-\frac {a+b}2\right )^2\frac {\d x}{b-a}\\ &amp;=\frac {1}{3(b-a)}\left (x-\frac {a+b}2\right )^3\bigg |_a^b\\ &amp;=\frac {1}{24(b-a)}\left
((b-a)^3-(a-b)^3\right )\\ &amp;=\frac {(b-a)^2}{12}\qedhere
\end{align*}


</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-159"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The uniform distribution is often too simple to be useful, but may be helpful for the simulation of random numbers, since \(U(0,1)\) may be transformed into almost any other distribution.
</p>

</li>

</ul>

</div>
<h6 id="autosec-160"><span class="sectionnumber">1.5.2&#x2003;</span>Exponential</h6>
<a id="MATH0057-autopage-160"></a>


<a id="sec:exp"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-161"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Exponential distribution)</span>.   For incidents which occur at a Poisson rate \(\lambda \), the length of the interval \(X\) until the first incident occurs is distributed
with (negative) exponential distribution
</p>

<p>
\[ X\sim \Exp (\lambda ) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-162"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.30</span></span>.   \(X\sim \Exp (\lambda )\) has pdf
</p>

<p>
\[ f(x)=\begin {cases}\lambda e^{-\lambda x}&amp;x&gt;0\\0&amp;\text {otherwise}\end {cases} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-163"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> The number of incidents \(N(t)\) occurring in an interval \((0,t)\) is distributed \(N(t)\sim \Poi (\lambda t)\), so cdf of \(X\) at x is
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{35}\)</span>


<!--



                                                                                                                     F (x) = P (X ≤ x)
                                                                                                                                                   
                                                                                                                          = P incident in (0, x]
                                                                                                                                         
                                                                                                                          = P N (x) ≥ 1
                                                                                                                                              
                                                                                                                          = 1 − P N (x) = 0
                                                                                                                                (λx)0 e−λx
                                                                                                                          =1−
                                                                                                                                    0!
                                                                                                                          = 1 − e−λx



-->


<p>


\begin{align*}
F(x)&amp;=P(X\le x)\\ &amp;=P\big (\text {incident in }(0,x]\big )\\ &amp;=P\big (N(x)\ge 1\big )\\ &amp;=1-P\big (N(x)=0\big )\\ &amp;=1-\frac {(\lambda x)^0e^{-\lambda x}}{0!}\\ &amp;=1-e^{-\lambda x}
\end{align*}
Differentiating then gives the pdf. Check that this is a pdf.
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:pdf1">1.32</a>)</span> \(f(x)\ge 0\) everywhere is trivial.
</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:pdf2">1.33</a>)</span>
</p>
<p>
\[ \int _\R f(x)\;\d x=\int _0^\infty \lambda e^{\lambda x}\;\d x=-e^{-\lambda x}\big |_0^\infty =0-(-1)=1\qedhere \]
</p>
<p>


</p>
</li>
</ul>

<p>
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-164"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.31</span></span>.   The expectation and variance of \(X\sim \Exp (\lambda )\) are
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{35}\)</span>


<!--



                                                                                                                          1
                                                                                                                    E(X) =                                                                            (1.36)   --><a id="eq:Exp1"></a><!--
                                                                                                                          λ
                                                                                                                           1
                                                                                                                  Var(X) = 2                                                                          (1.37)   --><a id="eq:Exp2"></a><!--
                                                                                                                          λ


-->


<p>


\begin{align}
\label {eq:Exp1}E(X)&amp;=\frac 1\lambda \\ \label {eq:Exp2}\Var (X)&amp;=\frac 1{\lambda ^2}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-165"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Find the moment generating function for \(X\sim \Exp (\lambda )\).
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{37}\)</span>


<!--



                                                                                                                           M (t) = E(etX )
                                                                                                                                      Z
                                                                                                                                 =         etx f (x) dx
                                                                                                                                       R
                                                                                                                                          Z    ∞
                                                                                                                                 =λ                etx e−λx dx
                                                                                                                                           0
                                                                                                                                    e(t−λ)x           ∞
                                                                                                                                 =λ
                                                                                                                                     t−λ              0

This is only finite for \(t&lt;\lambda \). Since \(\lambda \) is fixed and positive, \(t&gt;0\) can always be picked such that this is the case.
                                                                                                                                                    1              λ
                                                                                                                                                         
                                                                                                                           M (t) = λ 0 −                      =
                                                                                                                                                   t−λ            λ−t
The moments can be found by differentiating with respect to \(t\).
                                                                                                                                       λ
                                                                                                                          M ′ (t) =
                                                                                                                                    (λ − t)2
                                                                                                                                       2λ
                                                                                                                         M ′′ (t) =
                                                                                                                                    (λ − t)3
Only \(n=1,2\) are needed for this proof, but the general case is included for completion.
                                                                                                                                         n!λ
                                                                                                                        M (n) (t) =
                                                                                                                                      (λ − t)r+1
Then
                                                                                                                                             1
                                                                                                                          E(X) = M ′ (0) =
                                                                                                                                             λ
                                                                                                                                               2
                                                                                                                         E(X 2 ) = M ′′ (0) = 2
                                                                                                                                              λ
                                                                                                                                                 n!
                                                                                                                         E(X n ) = M (n) (0) = n
                                                                                                                                                λ


-->


<p>


\begin{align*}
M(t)&amp;=E(e^{tX})\\ &amp;=\int _\R e^{tx}f(x)\;\d x\\ &amp;=\lambda \int _0^\infty e^{tx}e^{-\lambda x}\;\d x\\ &amp;=\lambda \frac {e^{(t-\lambda )x}}{t-\lambda }\Big |_0^\infty \intertext {This is only finite for $t&lt;\lambda $.
Since $\lambda $ is fixed and positive, $t&gt;0$ can always be picked such that this is the case.} M(t)&amp;=\lambda \left (0-\frac 1{t-\lambda }\right )=\frac \lambda {\lambda -t} \intertext {The moments can be found by differentiating
with respect to $t$.} M&apos;(t)&amp;=\frac {\lambda }{(\lambda -t)^2}\\ M&apos;&apos;(t)&amp;=\frac {2\lambda }{(\lambda -t)^3}\\ \intertext {Only $n=1,2$ are needed for this proof, but the general case is included for completion.}
M^{(n)}(t)&amp;=\frac {n!\lambda }{(\lambda -t)^{r+1}} \shortintertext {Then} E(X)&amp;=M&apos;(0)=\frac {1}{\lambda }\\ E(X^2)&amp;=M&apos;&apos;(0)=\frac 2{\lambda ^2}\\ E(X^n)&amp;=M^{(n)}(0)=\frac {n!}{\lambda ^n}
\end{align*}
So
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{37}\)</span>


<!--



                                                                                                                                                     1
                                                                                                                                      E(X) =                                                                                               (1.36)
                                                                                                                                                     λ
                                                                                                                                                                   2
                                                                                                                                                          2        1         1
                                                                                                                 Var(X) = E(X 2 ) − E(X)2 =                  −           =                                                                 (1.37)
                                                                                                                                                          λ2       λ         λ2


-->


<p>


\begin{gather*}
\tag *{\labelcref {eq:Exp1}}E(X)=\frac 1\lambda \\ \tag *{\labelcref {eq:Exp2}}\Var (X)=E(X^2)-E(X)^2=\frac 2{\lambda ^2}-\left (\frac 1\lambda \right )^2=\frac 1{\lambda ^2}
\end{gather*}
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-166"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.32</span></span>.   The median of \(X\sim \Exp (\lambda )\) is \(\frac {\ln 2}\lambda \), so such a distribution cannot have the same
median as mean.
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-167"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> For the median \(m\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{37}\)</span>


<!--



                                                                                                                               1
                                                                                                                          F (m) =
                                                                                                                               2
                                                                                                                         −λm   1
                                                                                                                     1−e     =
                                                                                                                               2
                                                                                                                               ln 2
                                                                                                                           m=
                                                                                                                                λ


-->


<p>


\begin{align*}
F(m)&amp;=\frac 12\\ 1-e^{-\lambda m}&amp;=\frac 12\\ m&amp;=\frac {\ln 2}\lambda
\end{align*}
Since the mean is \(\frac 1\lambda \), this cannot be equal to the median. <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-168"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.            The mean time between incidents is \(\frac 1\lambda \) for \(X\sim \Exp (\lambda )\). If the last incident was \(L\) units prior, what is the distribution of the interval until the next emission. In other words,
what is the distribution of \(X-L\) given that \(X&gt;L\)?
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{37}\)</span>


<!--


                                                                                                                                                                    
                                                                                                                                          P (X − L > x) ∩ (X > L)
                                                                                                              P (X − L > x | X > L) =
                                                                                                                                                 P (X > L)
                                                                                                                                                                   
                                                                                                                                          P (X > L + x) ∩ (X > L)
                                                                                                                                      =
                                                                                                                                                 P (X > L)
                                                                                                                                        P (X > L + x)
                                                                                                                                      =
                                                                                                                                          P (X > L)
                                                                                                                                          e−λ(L+x)
                                                                                                                                      =            = e−λx
                                                                                                                                            e−λL


-->


<p>


\begin{align*}
P(X-L&gt;x\mid X&gt;L)&amp;=\frac {P\big ((X-L&gt;x)\cap (X&gt;L)\big )}{P(X&gt;L)}\\ &amp;=\frac {P\big ((X&gt;L+x)\cap (X&gt;L)\big )}{P(X&gt;L)}\\ &amp;=\frac {P(X&gt;L+x)}{P(X&gt;L)}\\ &amp;=\frac {e^{-\lambda (L+x)}}{e^{-\lambda
L}}=e^{-\lambda x}
\end{align*}
Hence \(X-L\mid X&gt;L\sim \Exp (\lambda )\). The distribution of length of remaining interval until the next incident does not depend on the previous incident. This is called the lack of memory property, which is a result of independence between non-overlapping
intervals in Poisson process.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-169"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Just as the Poisson is the limit of a binomial distribution as \(n\to \infty \), for fixed \(np\), the exponential distribution is the limit of a geometric.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-170"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The exponential distribution is often used to model things like the times to failure of electrical components, or the lifespans of individuals.
</p>

</li>

</ul>

</div>
<h6 id="autosec-171"><span class="sectionnumber">1.5.3&#x2003;</span>Gamma</h6>
<a id="MATH0057-autopage-171"></a>


<a id="sec:gam"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-172"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.          Just as the exponential distribution may be seen as the limit of a geometric distribution, since it is the distribution of the interval until the first incident in a Poisson process, the gamma distribution may be
considered as the limit of the negative binomial distribution, and used to model the distribution of interval until the \(k^\text {th}\) incident.
</p>

<p>
The Poisson process and negative binomial both involve factorials, so clearly a continuous extension to this will be needed for the gamma distribution.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-173"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Gamma function)</span>.   The gamma function, for any \(\alpha &gt;0\), is
</p>

<p>
\[ \Gamma (\alpha )=\int _0^\infty x^{\alpha -1}e^{-x}\;\d x \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-174"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The Gamma function usually cannot be evaluated analytically, but it has several useful properties. It is called the generalised factorial, since it agrees with the factorial for \(\alpha \in \N \), but is also defined
for non-integers.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-175"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.33</span></span>.   <a id="thm:gam1"></a> It may be useful to know that
</p>

<p>
\[ \Gamma (\tfrac 12)=\sqrt {\pi } \]
</p>

<p>
Proof is in section <a href="MATH0057.html#sec:beta">1.5.4</a>.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-176"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.34</span></span>.
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{37}\)</span>


<!--



                                                                                                            ∀α > 0   Γ(α + 1) = αΓ(α)                                               (1.38)        --><a id="eq:gam1"></a><!--
                                                                                                            ∀α ∈ N      Γ(α) = (α − 1)!                                             (1.39)        --><a id="eq:gam2"></a><!--



-->


<p>


\begin{alignat} {2}
\label {eq:gam1} \forall \alpha &gt;0&amp;&amp;\quad \Gamma (\alpha +1)&amp;=\alpha \Gamma (\alpha )\\ \label {eq:gam2}\forall \alpha \in \N &amp;&amp;\quad \Gamma (\alpha )&amp;=(\alpha -1)!
\end{alignat}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-177"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:gam1">1.38</a>)</span> Integrate by parts. For \(\alpha &gt;1\).
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{39}\)</span>


<!--

                                                                                                                     Z    ∞
                                                                                                            Γ(α) =            xα−1 e−x dx
                                                                                                                      0
                                                                                                                                            Z    ∞
                                                                                                                               ∞
                                                                                                                =    −xα−1 e−x          +            (α − 1)xα−2 e−x dx
                                                                                                                               0
                                                                                                                                             0
                                                                                                                                   Z    ∞
                                                                                                                = 0 + (α − 1)               xα−2 e−x dx
                                                                                                                                    0

                                                                                                                = (α − 1)Γ(α − 1)


-->


<p>


\begin{align*}
\Gamma (\alpha )&amp;=\int _0^\infty x^{\alpha -1}e^{-x}\;\d x\\ &amp;=-x^{\alpha -1}e^{-x}\big |_0^\infty +\int _0^\infty (\alpha -1)x^{\alpha -2}e^{-x}\;\d x\\ &amp;=0+(\alpha -1)\int _0^\infty x^{\alpha -2}e^{-x}\;\d x\\ &amp;=(\alpha
-1)\Gamma (\alpha -1)
\end{align*}
Hence substituting \(\alpha +1\) with \(\alpha &gt;0\) in place of \(\alpha \) gives the required result.
</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:gam2">1.39</a>)</span> Proof by induction. For \(\alpha =1\),
</p>
<p>
\[ \Gamma (1)=\int _0^\infty e^{-x}\;\d x=\frac {e^{-x}}{-1}\bigg |_0^\infty =0-(-1)=1 \]
</p>
<p>
The recurrence relation <span class="textup">(<a href="MATH0057.html#eq:gam1">1.38</a>)</span> gives the result by induction.<span class="theoremendmark">&#x25A1;</span>
</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-178"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   Let \(X\) be the interval until the \(k^\text {th}\) accident in a Poisson process. Then \(X\) takes values in \(\R ^+\), and for \(x&gt;0\) has cdf
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{39}\)</span>


<!--



                                                                                                          F (x) = 1 − P (X > x)
                                                                                                                                                                         
                                                                                                                = 1 − P at most \(k-1\) incidents in (0, x]
                                                                                                                          k−1
                                                                                                                          X   eλx (λx)n
                                                                                                                =1−
                                                                                                                                       n!
                                                                                                                          n=0

Integrate to get the pdf.
                                                                                                                        k−1
                                                                                                                        X   1 d                      
                                                                                                  f (x) = F ′ (x) = −                   e−λx (λx)n
                                                                                                                               n! dx
                                                                                                                        n=0
                                                                                                                        k−1
                                                                                                                            1                d −λx           d
                                                                                                                        X                                                   
                                                                                                                =−                  (λx)n      (e  ) + e−λx    (λx)n
                                                                                                                               n!           dx              dx
                                                                                                                        n=0
                                                                                                                    k−1                        k−1
                                                                                                                    X   1                      X   1
                                                                                                                =              λ(λx)n e−x −               λne−λx (λx)n−1
                                                                                                                          n!                         n!
                                                                                                                    n=0                        n=1

Substitute \(\ell =n-1\) into the second sum.
                                                                                                                    k−1                        k−2
                                                                                                                    X   1                      X   1
                                                                                                                =              λ(λx)n e−x −               λne−λx (λx)ℓ
                                                                                                                          n!                         ℓ!
                                                                                                                    n=0                        ℓ=0
                                                                                                                  λk xk−1 e−λx
                                                                                                                =
                                                                                                                     (k − 1)!


-->


<p>


\begin{align*}
F(x)&amp;=1-P(X&gt;x)\\&amp;=1-P\big (\text {at most $k-1$ incidents in }(0,x]\big )\\ &amp;=1-\sum _{n=0}^{k-1}\frac {e^{\lambda x}(\lambda x)^n}{n!} \intertext {Integrate to get the pdf.} f(x)=F&apos;(x)&amp;=-\sum _{n=0}^{k-1}\frac
1{n!}\ddx {}\big (e^{-\lambda x}(\lambda x)^n\big )\\ &amp;=-\sum _{n=0}^{k-1}\frac 1{n!}\left ((\lambda x)^n\ddx {}(e^{-\lambda x})+e^{-\lambda x}\ddx {}(\lambda x)^n\right )\\ &amp;=\sum _{n=0}^{k-1}\frac 1{n!}\lambda (\lambda
x)^ne^{-x}-\sum _{n=1}^{k-1}\frac 1{n!}\lambda ne^{-\lambda x}(\lambda x)^{n-1} \intertext {Substitute $\ell =n-1$ into the second sum.} &amp;=\sum _{n=0}^{k-1}\frac 1{n!}\lambda (\lambda x)^ne^{-x}-\sum _{\ell =0}^{k-2}\frac 1{\ell
!}\lambda ne^{-\lambda x}(\lambda x)^{\ell }\\ &amp;=\frac {\lambda ^kx^{k-1}e^{-\lambda x}}{(k-1)!}
\end{align*}
This motivates the following definition.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-179"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Gamma distribution)</span>.   A continuous random variable \(X\) has a gamma distribution with parameters \(a,\lambda &gt;0\), written \(X\sim \Gamma (\alpha
,\lambda )\), if it has pdf
</p>

<p>
\[ f(x)=\begin {cases} \displaystyle \frac {\lambda ^\alpha x^{\alpha -1}e^{-\lambda x}}{\Gamma (\alpha )}&amp;x\ge 0\\0&amp;\text {otherwise} \end {cases} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-180"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.35</span></span>.   This is a valid pdf.
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-181"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> This is a valid pdf since it is non-negative everywhere, and
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{39}\)</span>


<!--


                                                                                                           Z                       Z    ∞
                                                                                                                             λα
                                                                                                               f (x) dx =                   xα−1 e−λx dx
                                                                                                           R
                                                                                                                            Γ(α)    0
Substitute \(u=\lambda x\).
                                                                                                                                   Z    ∞    α−1
                                                                                                                             λα              u             du
                                                                                                                       =                             e−u
                                                                                                                            Γ(α)             λ             λ
                                                                                                                                   Z0 ∞
                                                                                                                          1
                                                                                                                       =                    uα−1 e−u du
                                                                                                                         Γ(α)       0
                                                                                                                            Γ(α)
                                                                                                                       =         =1
                                                                                                                            Γ(α)


-->


<p>


\begin{align*}
\int _\R f(x)\;\d x&amp;=\frac {\lambda ^\alpha }{\Gamma (\alpha )}\int _0^\infty x^{\alpha -1}e^{-\lambda x}\;\d x \shortintertext {Substitute $u=\lambda x$.} &amp;=\frac {\lambda ^\alpha }{\Gamma (\alpha )}\int _0^\infty \left (\frac
{u}\lambda \right )^{\alpha -1}e^{-u}\frac {\d u}\lambda \\ &amp;=\frac {1}{\Gamma (\alpha )}\int _0^\infty u^{\alpha -1}e^{-u}\;\d u\\ &amp;=\frac {\Gamma (\alpha )}{\Gamma (\alpha )}=1\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-182"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   As should be expected, \(\Gamma (1,\lambda )\) is the same distribution as \(\Exp (\lambda )\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-183"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.       The gamma distribution is useful in many more situations than that which motivated it, due to the flexible range of shapes it offers. \(\alpha \) is called the index or shape parameter, and \(\lambda \) is called
the scale parameter, or sometimes just the parameter.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-184"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.36</span></span>.   The expectation and variance of \(X\sim \Gamma (\alpha ,\lambda )\) are
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{39}\)</span>


<!--



                                                                                                                          α
                                                                                                                    E(X) =                                                                               (1.40)   --><a id="eq:gam3"></a><!--
                                                                                                                          λ
                                                                                                                           α
                                                                                                                  Var(X) = 2                                                                             (1.41)   --><a id="eq:gam4"></a><!--
                                                                                                                          λ


-->


<p>


\begin{align}
\label {eq:gam3}E(X)&amp;=\frac \alpha \lambda \\ \label {eq:gam4}\Var (X)&amp;=\frac \alpha {\lambda ^2}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-185"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Find the moment generating function for \(X\sim \Gamma (\alpha ,\lambda )\).
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{41}\)</span>


<!--



                                                                                                                 M (t) = E(etX )
                                                                                                                            Z
                                                                                                                       =         etx f (x) dx
                                                                                                                             R
                                                                                                                            Z    ∞
                                                                                                                                           λα xα−1 e−λx
                                                                                                                       =             etx                dx
                                                                                                                             0
                                                                                                                                               Γ(α)
                                                                                                                                     Z     ∞
                                                                                                                          λα
                                                                                                                       =                       xα−1 e−(λ−t)x dx
                                                                                                                         Γ(α)          0
                                                                                                                                                         Z   ∞                  
                                                                                                                            λα                 (λ − t)α           α−1 −(λ−t)x
                                                                                                                       =                                          x   e         dx
                                                                                                                         (λ − t)α                Γ(α)     0

For \(t&lt;\lambda \), the bracketed expression is the integral of the pdf of the distribution \(X\sim \Gamma (\alpha ,\lambda -t)\), so evaluates to 0. Since \(\lambda &gt;0\), such a \(t\) can always be found, hence
                                                                                                                               λα
                                                                                                                 M (t) =
                                                                                                                            (λ − t)α
The moments can be found by differentiating with respect to \(t\).
                                                                                                                              αλα
                                                                                                                M ′ (t) =
                                                                                                                           (λ − t)α+1
                                                                                                                           α(α + 1)λα
                                                                                                                M ′′ (t) =
                                                                                                                           (λ − t)α+2
Then
                                                                                                                               α
                                                                                                                E(X) = M ′ (0) =
                                                                                                                               λ
                                                                                                                  2     ′′     α(α + 1)
                                                                                                               E(X ) = M (0) =
                                                                                                                                  λ2




-->


<p>


\begin{align*}
M(t)&amp;=E(e^{tX})\\ &amp;=\int _\R e^{tx}f(x)\;\d x\\ &amp;=\int _0^\infty e^{tx}\frac {\lambda ^\alpha x^{\alpha -1}e^{-\lambda x}}{\Gamma (\alpha )}\;\d x\\ &amp;=\frac {\lambda ^\alpha }{\Gamma (\alpha )}\int _0^\infty x^{\alpha
-1}e^{-(\lambda -t) x}\;\d x\\ &amp;=\frac {\lambda ^\alpha }{(\lambda -t)^\alpha }\!\left (\frac {(\lambda -t)^\alpha }{\Gamma (\alpha )}\!\int _0^\infty \!x^{\alpha -1}e^{-(\lambda -t) x}\d x\!\right
)\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!        \intertext {For $t&lt;\lambda $, the bracketed expression is the integral of the pdf of the distribution $X\sim \Gamma (\alpha ,\lambda -t)$, so evaluates to 0.   Since $\lambda &gt;0$,
such a $t$ can always be found, hence} M(t)&amp;=\frac {\lambda ^\alpha }{(\lambda -t)^\alpha } \intertext {The moments can be found by differentiating with respect to $t$.} M&apos;(t)&amp;=\frac {\alpha \lambda ^\alpha }{(\lambda
-t)^{\alpha +1}}\\ M&apos;&apos;(t)&amp;=\frac {\alpha (\alpha +1)\lambda ^\alpha }{(\lambda -t)^{\alpha +2}}\\ \shortintertext {Then} E(X)&amp;=M&apos;(0)=\frac \alpha \lambda \\ E(X^2)&amp;=M&apos;&apos;(0)=\frac {\alpha (\alpha
+1)}{\lambda ^2}\\
\end{align*}
So
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{41}\)</span>


<!--



                                                                                                                                   α
                                                                                                                                      E(X) =                                                                                                   (1.40)
                                                                                                                                   λ
                                                                                                                                              2
                                                                                                                      2        2  α(α + 1)    α     α
                                                                                                          Var(X) = E(X ) − E(X) =          −      = 2                                                                                          (1.41)
                                                                                                                                     λ2       λ    λ


-->


<p>


\begin{gather*}
\tag *{\labelcref {eq:gam3}}E(X)=\frac \alpha \lambda \\ \tag *{\labelcref {eq:gam4}}\Var (X)=E(X^2)-E(X)^2=\frac {\alpha (\alpha +1)}{\lambda ^2}-\left (\frac \alpha \lambda \right )^2=\frac \alpha {\lambda ^2}
\end{gather*}
<span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<h6 id="autosec-186"><span class="sectionnumber">1.5.4&#x2003;</span>Beta</h6>
<a id="MATH0057-autopage-186"></a>


<a id="sec:beta"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-187"></a>

<p>
<span class="amsthmnamedefinition">Definition</span>.   The beta function, for \(\alpha ,\beta &gt;0\), is
</p>

<p>
\[ \B (\alpha ,\beta )=\int _0^1x^{\alpha -1}(1-x)^{\beta -1}\;\d x \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-188"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.37</span></span>.
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{41}\)</span>


<!--



                                                                                                                                   Γ(α)Γ(β)
                                                                                                                         B(α, β) =                                 (1.42)                                         --><a id="eq:beta1"></a><!--
                                                                                                                                   Γ(α + β)
                                                                                                                                   (α − 1)!(β − 1)!
                                                                                                   ∀α, β ∈ N, a, b > 1   B(α, β) =                                 (1.43)                                         --><a id="eq:beta2"></a><!--
                                                                                                                                     (α + β − 1)!


-->


<p>


\begin{align}
\label {eq:beta1}\B (\alpha ,\beta )&amp;=\frac {\Gamma (\alpha )\Gamma (\beta )}{\Gamma (\alpha +\beta )}\\ \label {eq:beta2}\forall \alpha ,\beta \in \N ,\ a,b&gt;1 \quad \B (\alpha ,\beta )&amp;=\frac {(\alpha -1)!(\beta -1)!}{(\alpha
+\beta -1)!}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-189"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{43}\)</span>


<!--


                                                                                                                      Z   ∞                     Z   ∞
                                                                                                         Γ(α)Γ(β) =           e−u uα−1 du ·             e−v v β−1 dv
                                                                                                                      u=0                         v=0
                                                                                                                      Z∞      Z   ∞
                                                                                                                  =                     e−u−v uα−1 v β−1 du dv
                                                                                                                      v=0         u=0

Let \(u=zt,\ v=z(1-t)\). This has Jacobian

\[ \begin {pmatrix} u_z&amp;u_t\\ v_z&amp;v_t \end {pmatrix} = \begin {pmatrix} t&amp;z\\ 1-t&amp;-z \end {pmatrix} =-z \]

So \(\d u\;\d v=z\;\d z\;\d t\). \(z=u+v\) and \(t=\frac {u}{u+v}\), so the limits of integration for \(z\) are \(0\) to \(\infty \), and for \(t\) are \(0\) to \(1\). So
                                                                                                                      Z   ∞   Z   1
                                                                                                                                                                β−1
                                                                                                         Γ(α)Γ(β) =                     e−z (zt)α−1 z(1 − t)           z dt dz
                                                                                                                      z=0      t=0
                                                                                                                      Z∞                            Z   1
                                                                                                                  =           e−z z α+β−1 dz ·              tα−1 (1 − t)β−1 dt
                                                                                                                      z=0                           t=0

                                                                                                                  = Γ(α + β) B(α, β)



-->


<p>


\begin{align*}
\Gamma (\alpha )\Gamma (\beta )&amp;=\int _{u=0}^\infty e^{-u}u^{\alpha -1}\;\d u\cdot \int _{v=0}^\infty e^{-v}v^{\beta -1}\;\d v\\ &amp;=\int _{v=0}^\infty \int _{u=0}^\infty e^{-u-v}u^{\alpha -1}v^{\beta -1}\;\d u\;\d v \intertext {Let
$u=zt,\ v=z(1-t)$.    This has Jacobian \[ \begin{pmatrix} u_z&amp;u_t\\ v_z&amp;v_t \end {pmatrix} = \begin{pmatrix} t&amp;z\\ 1-t&amp;-z \end {pmatrix} =-z \] So $\d u\;\d v=z\;\d z\;\d t$.        $z=u+v$ and $t=\frac {u}{u+v}$, so the limits
of integration for $z$ are $0$ to $\infty $, and for $t$ are $0$ to $1$.          So} \Gamma (\alpha )\Gamma (\beta )&amp;=\int _{z=0}^\infty \int _{t=0}^1 e^{-z}(zt)^{\alpha -1}\big (z(1-t)\big )^{\beta -1}z\;\d t\;\d z\\ &amp;=\int
_{z=0}^\infty e^{-z}z^{\alpha +\beta -1}\;\d z\cdot \int _{t=0}^1 t^{\alpha -1}(1-t)^{\beta -1}\;\d t\\ &amp;=\Gamma (\alpha +\beta )\B (\alpha ,\beta )
\end{align*}
Hence equation <span class="textup">(<a href="MATH0057.html#eq:beta1">1.42</a>)</span> is true. equation <span class="textup">(<a href="MATH0057.html#eq:beta2">1.43</a>)</span> is immediate from equations <span class="textup">(<a
href="MATH0057.html#eq:gam2">1.39</a>)</span> and <span class="textup">(<a href="MATH0057.html#eq:beta1">1.42</a>)</span>. <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-190"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Beta distribution)</span>.       A continuous random variable \(X\) has a beta distribution with parameters \(\alpha ,\beta &gt;0\), written \(X\sim \B (\alpha ,\beta
)\), if it has pdf
</p>

<p>
\[ f(x)=\begin {cases} \displaystyle \frac {x^{\alpha -1}(1-x)^{\beta -1}}{\B (\alpha ,\beta )}&amp;0&lt;x&lt;1\\0&amp;\text {otherwise} \end {cases} \]
</p>

<p>
This is a valid pdf since it is non-negative everywhere, and integrating gives 1 by the definition of the beta function
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-191"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.38</span></span>.     The expectation and variance of \(X\sim \B (\alpha ,\beta )\) are
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{43}\)</span>


<!--



                                                                                                                       α
                                                                                                            E(X) =                                                                        (1.44)            --><a id="eq:beta3"></a><!--
                                                                                                                      α+β
                                                                                                                               αβ
                                                                                                           Var(X) =                                                                       (1.45)            --><a id="eq:beta4"></a><!--
                                                                                                                      (α + β)2 (α + β + 1)


-->


<p>


\begin{align}
\label {eq:beta3}E(X)&amp;=\frac \alpha {\alpha +\beta }\\ \label {eq:beta4}\Var (X)&amp;=\frac {\alpha \beta }{(\alpha +\beta )^2(\alpha +\beta +1)}
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-192"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> The \(r^\text {th}\) moment of \(X\) about 0 is
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{45}\)</span>


<!--


                                                                                                                    Z    1
                                                                                                            1
                                                                                              E(X r ) =                      xr xα−1 (1 − x)β−1 dx
                                                                                                          B(α, β)    0
                                                                                                       B(α + r, β)
                                                                                                     =
                                                                                                         B(α, β)
                                                                                                       Γ(α + r)Γ(β)Γ(α + β)
                                                                                                     =
                                                                                                       Γ(α)Γ(β)Γ(α + β + r)
                                                                                                              α(α + 1) · · · (α + r − 1)Γ(α)Γ(α + β)
                                                                                                     =
                                                                                                       Γ(α)(α + β)(α + β + 1) · · · (α + β + r − 1)Γ(α + β)
                                                                                                              α(α + 1) · · · (α + r − 1)
                                                                                                     =
                                                                                                       (α + β)(α + β + 1) · · · (α + β + r − 1)
                                                                                                          r−1
                                                                                                          Y      α+i
                                                                                                     =
                                                                                                                α+β+i
                                                                                                          i=0



-->


<p>


\begin{align*}
E(X^r)&amp;=\frac 1{\B (\alpha ,\beta )}\int _0^1 x^rx^{\alpha -1}(1-x)^{\beta -1}\;\d x\\ &amp;=\frac {\B (\alpha +r,\beta )}{\B (\alpha ,\beta )}\\ &amp;=\frac {\Gamma (\alpha +r)\Gamma (\beta )\Gamma (\alpha +\beta )}{\Gamma (\alpha
)\Gamma (\beta )\Gamma (\alpha +\beta +r)}\\ &amp;=\frac {\alpha (\alpha +1)\cdots (\alpha +r-1)\Gamma (\alpha )\Gamma (\alpha +\beta )}{\Gamma (\alpha )(\alpha +\beta )(\alpha +\beta +1)\cdots (\alpha +\beta +r-1)\Gamma (\alpha +\beta
)}\\ &amp;=\frac {\alpha (\alpha +1)\cdots (\alpha +r-1)}{(\alpha +\beta )(\alpha +\beta +1)\cdots (\alpha +\beta +r-1)}\\ &amp;=\prod _{i=0}^{r-1}\frac {\alpha +i}{\alpha +\beta +i}
\end{align*}
The expectation follows immediately, and
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{45}\)</span>


<!--



                                                                                                      Var(X) = E(X 2 ) − E(X)2
                                                                                                                          α(α + 1)           α2
                                                                                                                 =                      −
                                                                                                                     (α + β)(α + β + 1)   (α + β)2
                                                                                                                   α(α + 1)(α + β) − α2 (α + β + 1)
                                                                                                                 =
                                                                                                                         (α + β)2 (α + β + 1)
                                                                                                                            αβ
                                                                                                                 =
                                                                                                                   (α + β)2 (α + β + 1)


-->


<p>


\begin{align*}
\Var (X)&amp;=E(X^2)-E(X)^2\\ &amp;=\frac {\alpha (\alpha +1)}{(\alpha +\beta )(\alpha +\beta +1)}-\frac {\alpha ^2}{(\alpha +\beta )^2}\\ &amp;=\frac {\alpha (\alpha +1)(\alpha +\beta )-\alpha ^2(\alpha +\beta +1)}{(\alpha +\beta
)^2(\alpha +\beta +1)}\\ &amp;=\frac {\alpha \beta }{(\alpha +\beta )^2(\alpha +\beta +1)}\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-193"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   The beta family is extremely flexible. It might be expected to provide a good model for the distribution of any random variable which arises as a proportion (the integrand looks like a binomial).
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-194"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof of Proposition <a href="MATH0057.html#thm:gam1">1.33</a>.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{45}\)</span>


<!--


                                                                                                                        Z   1
                                                                                                                                    1                    1
                                                                                                       B( 12 , 12 ) =           x 2 −1 (1 − x) 2 −1 dx
                                                                                                                        0
                                                                                                                        Z   1
                                                                                                                                  dx
                                                                                                                  =             √ √
                                                                                                                        0
                                                                                                                                 x 1−x

Substitute \(u=\sqrt {x}\). Then \(\d u=\frac 1{2u}\;\d x\), so
                                                                                                                        Z   1
                                                                                                                                     2u
                                                                                                                  =             √          du
                                                                                                                        0
                                                                                                                                    1 − xu
                                                                                                                            Z   1
                                                                                                                                            du
                                                                                                                  =2                √
                                                                                                                            0            1 − u2
                                                                                                                                                            b
                                                                                                                  =2            lim              arcsin(u)
                                                                                                                                                              a
                                                                                                                                        −
                                                                                                                            b→1
                                                                                                                            a→0+

                                                                                                                  =π



-->


<p>


\begin{align*}
B(\tfrac 12,\tfrac 12)&amp;=\int _0^1 x^{\frac 12-1}(1-x)^{\frac 12-1}\;\d x\\ &amp;=\int _0^1 \frac {\d x}{\sqrt {x}\sqrt {1-x}} \intertext {Substitute $u=\sqrt {x}$.   Then $\d u=\frac 1{2u}\;\d x$, so} &amp;=\int _0^1 \frac {2u}{\sqrt
{1-x}u}\;\d u\\ &amp;=2\int _0^1 \frac {\d u}{\sqrt {1-u^2}}\\ &amp;=2\lim _{\scriptstyle \begin{array}{c}\scriptstyle b\to 1^-\\ [-4pt] \scriptstyle a\to 0^+\end {array}}\big [\arcsin (u)\big ]_a^b\\ &amp;=\pi
\end{align*}
So
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{45}\)</span>


<!--



                                                                                                                                        Γ( 21 )Γ( 12 )
                                                                                                                B( 12 , 12 ) =
                                                                                                                                        Γ( 12 + 21 )
                                                                                                                   Γ( 21 )2 = 1π
                                                                                                                              √
                                                                                                                    Γ( 21 ) = π



-->


<p>


\begin{align*}
B(\tfrac 12,\tfrac 12)&amp;=\frac {\Gamma (\tfrac 12)\Gamma (\tfrac 12)}{\Gamma (\tfrac 12+\tfrac 12)}\\ \Gamma (\tfrac 12)^2&amp;=1\pi \\ \Gamma (\tfrac 12)&amp;=\sqrt {\pi }\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<h6 id="autosec-195"><span class="sectionnumber">1.5.5&#x2003;</span>Normal</h6>
<a id="MATH0057-autopage-195"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-196"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Normal distribution)</span>.   A continuous random variable \(X\) has a normal (or Gaussian)distribution with parameters \(\mu ,\sigma ^2\), written \(X\sim \mathcal
{N}(\mu ,\sigma ^2)\), if it has pdf
</p>

<p>
\[ f(x)=\frac {\displaystyle e^{\displaystyle -\frac {(x-\mu )^2}{2\sigma ^2}}}{\displaystyle \sqrt {2\pi \sigma ^2}} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-197"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.39</span></span>.   This is a valid pdf.
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-198"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> Clearly it is non-negative.
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{45}\)</span>


<!--


                                                                                                                                         (x−µ)2
                                                                                             Z                      Z   ∞            −
                                                                                                             1                      e      2σ 2
                                                                                                 f (x) dx = √                            √                       dx
                                                                                               R              2π    −∞                       σ2

Substitute \(z=\frac {x-\mu }\sigma \).
                                                                                                                    Z   ∞
                                                                                                           1                             1       2
                                                                                                        = √                         e− 2 z dz
                                                                                                            2π      −∞
                                                                                                                        Z           ∞
                                                                                                           1                                     1       2
                                                                                                        = √    ·2                       e− 2 z dz
                                                                                                            2π                  0
                                                                                                            r Z                ∞                                Z    ∞                   21
                                                                                                                2                            1       2                      1    2
                                                                                                        =                            e− 2 z dz                            e− 2 y dy
                                                                                                                π           0                                     0
                                                                                                            r Z                ∞    Z       ∞                                        21
                                                                                                                2                                        −1 (y 2 +z 2 )
                                                                                                        =                                            e    2                 dz dy
                                                                                                                π           0            0

Substitute \(y=sz\).
                                                                                                            r Z                ∞    Z       ∞                                             21
                                                                                                                2                                            1    2 2
                                                                                                                                                                     z +z 2 )
                                                                                                        =                                            e− 2 (s                    z dz ds
                                                                                                                π           0            0
                                                                                                            r Z                ∞    Z       ∞                                            21
                                                                                                                2                                        −1 z 2 (1+s2 )
                                                                                                        =                                            e    2                 z dz ds
                                                                                                                π           0            0
                                                                                                            r Z                ∞                                                    i∞  21
                                                                                                                2                              1
                                                                                                                                     h                 1 2     2
                                                                                                        =                                −          e− 2 z (1+s )                         ds
                                                                                                                π           0
                                                                                                                                             1 + s2                                  0
                                                                                                            r Z                ∞                                 12
                                                                                                                2                         1
                                                                                                        =                                      ds
                                                                                                                π           0
                                                                                                                                        1 + s2
                                                                                                            r
                                                                                                                2                               ∞  12
                                                                                                        =               arctan(s)
                                                                                                                π                                    0
                                                                                                            r  1
                                                                                                                22      π
                                                                                                        =                               =1
                                                                                                                π       2


-->


<p>


\begin{align*}
\int _\R f(x)\;\d x&amp;=\frac 1{\sqrt {2\pi }}\int _{-\infty }^\infty \frac { e^{ -\frac {(x-\mu )^2}{2\sigma ^2}}}{\sqrt {\sigma ^2}}\;\d x \intertext {Substitute $z=\frac {x-\mu }\sigma $.} &amp;=\frac 1{\sqrt {2\pi }}\int _{-\infty
}^\infty e^{-\frac 12z^2}\;\d z\\ &amp;=\frac 1{\sqrt {2\pi }}\cdot 2\int _0^\infty e^{-\frac 12z^2}\;\d z\\ &amp;=\sqrt {\frac 2\pi }\left (\int _0^\infty e^{-\frac 12z^2}\;\d z\int _0^\infty e^{-\frac 12y^2}\;\d y\right )^\frac 12\\
&amp;=\sqrt {\frac 2\pi }\left (\int _0^\infty \int _0^\infty e^{-\frac 12(y^2+z^2)}\;\d z\;\d y\right )^\frac 12 \intertext {Substitute $y=sz$.} &amp;=\sqrt {\frac 2\pi }\left (\int _0^\infty \int _0^\infty e^{-\frac 12(s^2z^2+z^2)}z\;\d
z\;\d s \right )^\frac 12\\ &amp;=\sqrt {\frac 2\pi }\left (\int _0^\infty \int _0^\infty e^{-\frac 12z^2(1+s^2)}z\;\d z\;\d s \right )^\frac 12\\ &amp;=\sqrt {\frac 2\pi }\left (\int _0^\infty \left [ -\frac 1{1+s^2}e^{-\frac
12z^2(1+s^2)}\right ]_0^\infty \!\d s \right )^\frac 12\\ &amp;=\sqrt {\frac 2\pi }\left (\int _0^\infty \frac 1{1+s^2}\;\d s \right )^\frac 12\\ &amp;=\sqrt {\frac 2\pi }\left (\big [\arctan (s)\big ]_0^\infty \right )^\frac 12\\
&amp;=\sqrt {\frac 2\pi }\left (\frac \pi 2\right )^\frac 12=1\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-199"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   A normal distribution is symmetric about its mean, and is unimodal. It is often described as bell-shaped.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-200"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.40</span></span>.   The expectation and variance of \(X\sim \mathcal {N}(\mu ,\sigma ^2)\) are
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{45}\)</span>


<!--



                                                                                                                    E(X) = µ                                                                             (1.46)    --><a id="eq:nor1"></a><!--
                                                                                                                   Var(X) = σ   2
                                                                                                                                                                                                         (1.47)    --><a id="eq:nor2"></a><!--



-->


<p>


\begin{align}
\label {eq:nor1}E(X)&amp;=\mu \\ \label {eq:nor2}\Var (X)&amp;=\sigma ^2
\end{align}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-201"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:nor1">1.46</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--

                                                                                                   Z   ∞
                                                                                          E(X) =            xf (x) dx
                                                                                                    −∞
                                                                                                                Z   ∞             (x−µ)2
                                                                                                        1                    −
                                                                                               = √                       xe         2σ 2     dx
                                                                                                       2πσ 2        −∞
                                                                                                                                                                                    (x−µ)2
                                                                                                                Z   ∞                       (x−µ)2
                                                                                                                                                                    Z   ∞       −
                                                                                                        1                               −                                   e         2σ 2
                                                                                               = √                       (x − µ)e             2σ 2   dx + µ                     √            dx
                                                                                                       2πσ 2        −∞                                              −∞              2πσ 2
                                                                                                                                (x−µ)2      ∞
                                                                                                        1                   −
                                                                                               = √              (−σ 2 )e          2σ 2             +µ
                                                                                                       2πσ 2                                −∞

                                                                                               =µ


-->


<p>


\begin{align*}
E(X)&amp;=\int _{-\infty }^\infty xf(x)\;\d x\\ &amp;=\frac 1{\sqrt {2\pi \sigma ^2}}\int _{-\infty }^\infty xe^{ -\frac {(x-\mu )^2}{2\sigma ^2}}\;\d x\\ &amp;=\frac 1{\sqrt {2\pi \sigma ^2}}\int _{-\infty }^\infty (x-\mu )e^{ -\frac
{(x-\mu )^2}{2\sigma ^2}}\;\d x+\mu \int _{-\infty }^\infty \frac {e^{ -\frac {(x-\mu )^2}{2\sigma ^2}}}{\sqrt {2\pi \sigma ^2}}\;\d x\\ &amp;=\frac 1{\sqrt {2\pi \sigma ^2}}(-\sigma ^2)e^{ -\frac {(x-\mu )^2}{2\sigma ^2}}\bigg |_{-\infty
}^\infty +\mu \\ &amp;=\mu
\end{align*}


</p>


</li>
<li>
<p>
<span class="textup">(<a href="MATH0057.html#eq:nor2">1.47</a>)</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--

                                                                                                                                    
                                                                                               Var(X) = E (X − µ)2
                                                                                                                Z   ∞                                    (x−µ)2
                                                                                                                              1                      −
                                                                                                            =            √           (x − µ)2 e            2σ 2     dx
                                                                                                                 −∞          2πσ 2

Substitute \(t=\frac {x-\mu }\sigma \).
                                                                                                                        Z    ∞
                                                                                                               1                              t2
                                                                                                            = √                   σ 2 t2 e−    2   dt
                                                                                                                2π          −∞

Use integration by parts.
                                                                                                                                        i∞                  Z   ∞
                                                                                                               σ2     −t2                                            1  t2
                                                                                                                        h
                                                                                                            = √    −te 2                         +σ     2
                                                                                                                                                                    √ e− 2 dt
                                                                                                                2π                          −∞              −∞       2π

                                                                                                            = 0 + σ2 = σ2


-->


<p>


\begin{align*}
\Var (X)&amp;=E\left ((X-\mu )^2\right )\\ &amp;=\int _{-\infty }^\infty \frac 1{\sqrt {2\pi \sigma ^2}}(x-\mu )^2e^{ -\frac {(x-\mu )^2}{2\sigma ^2}}\;\d x \intertext {Substitute $t=\frac {x-\mu }\sigma $.} &amp;=\frac 1{\sqrt {2\pi
}}\int _{-\infty }^\infty \sigma ^2t^2e^{-\frac {t^2}2}\;\d t \intertext {Use integration by parts.} &amp;=\frac {\sigma ^2}{\sqrt {2\pi }}\left [-te^\frac {-t^2}2\right ]_{-\infty }^\infty +\sigma ^2\int _{-\infty }^\infty \frac 1{\sqrt
{2\pi }}e^{-\frac {t^2}2}\;\d t\\ &amp;=0+\sigma ^2=\sigma ^2\qedhere
\end{align*}


</p>
</li>
</ul>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-202"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Standard normal)</span>.   The standard normal distribution \(\mathcal {N}(\mu ,\sigma ^2)\) has pdf \(\phi (x)\) and cdf \(\Phi (x)\) of
</p>

<p>
\[ \phi (x)=\frac 1{\sqrt {2\pi }}e^{-\frac 12x^2},\quad \Phi (x)=\int _{-\infty }^x \frac 1{\sqrt {2\pi }}e^{\frac 12u^2}\;\d u \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-203"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.41</span></span>.   Any normal distribution \(\mathcal {N}\sim (0,1)\) and the standard normal distribution \(Z\sim \mathcal
{N}(0,1)\) are related by
</p>

<p>
\[ Z=\frac {X-\mu }\sigma \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-204"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> For \(Z=g(X)=\frac {X-\mu }\sigma \), \(X=g^{-1}(Z)=\sigma Z+\mu \). \(\sigma \) is positive, so \(g(X)\) is strictly increasing. So
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--



                                                                                                                            1              (g −1 (z)−µ)2   dg −1 (z)
                                                                                                                                       −
                                                                                                              fZ (z) = √           e            2σ 2
                                                                                                                           2πσ 2                              dz
                                                                                                                            1              (σz+µ−µ)2   d(σz + µ)
                                                                                                                                       −
                                                                                                                    = √            e          2σ 2
                                                                                                                           2πσ 2                          dz
                                                                                                                         1      z2
                                                                                                                    = √      e− 2 σ
                                                                                                                       2πσ 2
                                                                                                                       1     z2
                                                                                                                    = √ e− 2 = ϕ(z)
                                                                                                                       2π


-->


<p>


\begin{align*}
f_Z(z)&amp;=\frac 1{\sqrt {2\pi \sigma ^2}} e^{-\frac {(g^{-1}(z)-\mu )^2}{2\sigma ^2}} \frac {\d g^{-1}(z)}{\d z}\\ &amp;=\frac 1{\sqrt {2\pi \sigma ^2}} e^{-\frac {(\sigma z+\mu -\mu )^2}{2\sigma ^2}} \frac {\d (\sigma z+\mu )}{\d z}\\
&amp;=\frac 1{\sqrt {2\pi \sigma ^2}} e^{-\frac {z^2}2} \sigma \\ &amp;=\frac 1{\sqrt {2\pi }} e^{-\frac {z^2}2}=\phi (z)
\end{align*}
Hence \(Z\) has a standard normal distribution. <span class="theoremendmark">&#x25A1;</span>
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-205"></a>

<p>
<span class="amsthmnamedefinition">Method</span>.        For \(X\sim \mathcal {N}(\mu ,\sigma ^2)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--



                                                                                                                                a−µ    X −µ    b−µ
                                                                                                                                                    
                                                                                                             P (a < X < b) = P       <      <
                                                                                                                                 σ       σ      σ
                                                                                                                                a−µ        b−µ
                                                                                                                                              
                                                                                                                           =P        <Z<
                                                                                                                                 σ          σ
                                                                                                                                b−µ        a−µ
                                                                                                                                            
                                                                                                                           =Φ         −Φ
                                                                                                                                 σ          σ


-->


<p>


\begin{align*}
P(a&lt;X&lt;b)&amp;=P\left (\frac {a-\mu }\sigma &lt;\frac {X-\mu }\sigma &lt;\frac {b-\mu }\sigma \right )\\ &amp;=P\left (\frac {a-\mu }\sigma &lt;Z&lt;\frac {b-\mu }\sigma \right )\\ &amp;=\Phi \left (\frac {b-\mu }\sigma \right )-\Phi
\left (\frac {a-\mu }\sigma \right )
\end{align*}
These values of \(\Phi \) can then be found in tables to calculate the probabilities. Also, use the median \(\Phi (0)=0.5\) and the symmetry
</p>

<p>
\[ \Phi (-x)=1-\Phi (x) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-206"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.         The normal distribution is important for several reasons.
</p>
<ul style="list-style-type:none">


<li>
<p>
1. Many natural phenomena are approximately normally distributed, and even more can be transformed to fit, such as taking logs or cubes.
</p>


</li>
<li>
<p>
2. Normal distribution provides a good model for random measurement errors in experiments.
</p>


</li>
<li>
<p>
3. Many useful normal approximations to other distributions exist, such as binomial, Poisson and gamma distributions.
</p>


</li>
<li>
<p>
4. Linear combinations of normal distributions are also normal.
</p>


</li>
<li>
<p>
5. The distribution of the sum of a large number of independent random variable from any distribution, non of which predominate, tends in the limit to a normal distribution (see ??).
</p>
</li>
</ul>

</li>

</ul>

</div>
<h5 id="autosec-207"><span class="sectionnumber">1.6&#x2003;</span>Joint probability distributions</h5>
<a id="MATH0057-autopage-207"></a>
<h6 id="autosec-208"><span class="sectionnumber">1.6.1&#x2003;</span>Definition</h6>
<a id="MATH0057-autopage-208"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-209"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Joint distribution function)</span>.   For random variables \(X_1,\dots ,X_n\) defined on the same sample space, of \(X_1,\dots ,X_n\) is the function
</p>

<p>
\[ F_(x_1,\dots ,x_n)=P(X_1\le x_1,\dots ,X_n\le x_n) \]
</p>

<p>
where the notation \(X_1\le x_1,\dots ,X_n\le x_n\) means the event
</p>

<p>
\[ \{\omega :X_1(\omega )\le x_1\}\cap \cdots \cap \{\omega :X_n(\omega )\le x_n\} \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-210"></a>

<p>
<span class="amsthmnamedefinition">Definition</span>.   The distribution function of a single random variable is called its marginal distribution function, and can be derived from the joint distribution.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-211"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.       Joint distribution functions have the properties
</p>
<ul style="list-style-type:none">


<li>
<p>
− If any argument is \(-\infty \), then \(F(x_1,\dots ,x_n)=0\)
</p>


</li>
<li>
<p>
− \(F(\infty ,\dots ,\infty )=1\)
</p>


</li>
<li>
<p>
− \(F(x_1,\dots ,x_n)\) is non-decreasing in each of its arguments.
</p>
</li>
</ul>

</li>

</ul>

</div>
<h6 id="autosec-212"><span class="sectionnumber">1.6.2&#x2003;</span>Discrete</h6>
<a id="MATH0057-autopage-212"></a>


<a id="sec:joint discrete"></a>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-213"></a>

<p>
<span class="amsthmnamedefinition">Definition</span>.   For discrete random variables \(X_1,\dots X_n\), the joint probability mass function (joint pmf) is
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-214"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.42</span></span>.   For a discrete bivariate distribution,
</p>

<p>
\[ P(X_1=x_1)=\sum _{x_2}p(x_1,x_2) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-215"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--


                                                                                                                                                     !
                                                                                                                                     [
                                                                                                 P (X1 = x1 ) = P     {X1 = x1 } ∩        {X2 = x2 }
                                                                                                                                     x2
                                                                                                                                                 !
                                                                                                                      [
                                                                                                             =P            {X1 = x1 ∩ X2 = x2 }
                                                                                                                      x2
                                                                                                                 X
                                                                                                             =        P (X1 = x1 , X2 = x2 )
                                                                                                                 x2
                                                                                                                 X
                                                                                                             =        p(x1 , x2 )
                                                                                                                 x2



-->


<p>


\begin{align*}
P(X_1=x_1)&amp;=P\left (\{X_1=x_1\}\cap \bigcup _{x_2}\{X_2=x_2\}\right )\\ &amp;=P\left (\bigcup _{x_2}\{X_1=x_1\cap X_2=x_2\}\right )\\ &amp;=\sum _{x_2} P(X_1=x_1,\ X_2=x_2)\\ &amp;=\sum _{x_2}p(x_1,x_2)\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-216"></a>

<p>
<span class="amsthmnamedefinition">Remark</span>.   A discrete bivariate distribution may be written as a table. The row and column totals (or “margins”, hence the name) then give the marginal pmf of \(X_1\) and \(X_2\).
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-217"></a>

<p>
<span class="amsthmnamedefinition">Definition</span>.   The expectation of function of several variables \(\phi (X_1,\dots ,X_n):\Omega \to \R \) is
</p>

<p>
\[ E\big (\phi (X_1,\dots ,X_n)\big )=\sum _{x_1}\cdots \sum _{x_n}\phi (x_1,\dots ,x_n)p(x_1,\dots ,x_n) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-218"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.43</span></span>.   The covariance of random variables \(X_1,X_2\) with expectations \(\mu _1,\mu _2\) may be calculated
</p>

<p>
\[ \Cov (X_1,X_2)=E\big ((X_1-\mu _1)(X_2-\mu _2)\big )=E(X_1X_2)-\mu _1\mu _2 \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-219"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--


                                                                                                                XX
                                                                                    E (X1 − µ1 )(X2 − µ2 ) =                    (X1 − µ1 )(X2 − µ2 )p(x1 , x2 )
                                                                                                                 x1        x2
                                                                                                                 XX
                                                                                                             =                  (X1 X2 − X2 µ1 − X1 µ2 + µ1 µ2 )p(x1 , x2 )
                                                                                                                 x1        x2
                                                                                                                  XX                                                 XX
                                                                                                             =                        X1 X2 p(x1 , x2 ) + µ1 µ2                p(x1 , x2 )
                                                                                                                      x1        x2                                   x1   x2
                                                                                                                       XX                                       XX
                                                                                                              − µ1                        X2 p(x1 , x2 ) − µ2             X1 p(x1 , x2 )
                                                                                                                           x1        x2                         x1   x2

                                                                                                             = E(X1 X2 ) + µ1 µ2 · 1 − µ1 E(X2 ) − µ2 E(X1 )
                                                                                                             = E(X1 X2 ) − µ1 µ2



-->


<p>


\begin{align*}
E\big ((X_1-\mu _1)(X_2-\mu _2)\big )&amp;=\sum _{x_1}\sum _{x_2}(X_1-\mu _1)(X_2-\mu _2)p(x_1,x_2)\\ &amp;=\sum _{x_1}\sum _{x_2}(X_1X_2-X_2\mu _1-X_1\mu _2+\mu _1\mu _2)p(x_1,x_2)\\ &amp;=\phantom {-}\sum _{x_1}\sum
_{x_2}X_1X_2p(x_1,x_2)+\mu _1\mu _2\sum _{x_1}\sum _{x_2}p(x_1,x_2)\\ &amp;\phantom {=}-\mu _1\sum _{x_1}\sum _{x_2}X_2p(x_1,x_2)-\mu _2\sum _{x_1}\sum _{x_2}X_1p(x_1,x_2)\\ &amp;= E(X_1X_2)+\mu _1\mu _2\cdot 1-\mu _1E(X_2)-\mu _2E(X_1)\\
&amp;=E(X_1X_2)-\mu _1\mu _2\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-220"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.44</span></span>.   For random variables \(X_1,\dots ,X_n\),
</p>

<p>
\[ E\left (\sum _{i=1}^n X_i\right )=\sum _{i=1}^nE(X_i) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-221"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--


                                                                                                  n
                                                                                                             !
                                                                                                  X                  X           X
                                                                                              E         Xi       =         ···        (x1 + · · · + xn )p(x1 , . . . , xn )
                                                                                                  i=1                x1          xn
                                                                                                                     n
                                                                                                                                                                          !
                                                                                                                     X X                    X
                                                                                                                 =                    ···        xi p(x1 , . . . , xn )
                                                                                                                     i=1         x1         xn
                                                                                                                     n
                                                                                                                     X
                                                                                                                 =         E(Xi )
                                                                                                                     i=1



-->


<p>


\begin{align*}
E\left (\sum _{i=1}^n X_i\right )&amp;=\sum _{x_1}\cdots \sum _{x_n}(x_1+\cdots +x_n)p(x_1,\dots ,x_n)\\ &amp;=\sum _{i=1}^n\left (\sum _{x_1}\cdots \sum _{x_n}x_ip(x_1,\dots ,x_n)\right )\\ &amp;=\sum _{i=1}^nE(X_i)\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-222"></a>

<p>
<span class="amsthmnamedefinition">Definition</span>.   The following definition and results about independence immediately extend to more than two distributions.
</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-223"></a>

<p>
<span class="amsthmnamedefinition">Definition</span><span class="amsthmnotedefinition"> (Independence)</span>.   Discrete random variables \(X_1,X_2\) are independent if for all pairs \((x_1,x_2)\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--



                                                                                                  P (X1 = x1 and X2 = x2 ) = P (X1 = x1 )P (X2 = x2 )

so for marginal pmfs \(p_1,p_2\),

                                                                                                                  p(x1 , x2 ) = p(x1 )p(x2 )



-->


<p>


\begin{align*}
P(X_1=x_1\text { and }X_2=x_2)&amp;=P(X_1=x_1)P(X_2=x_2) \intertext {so for marginal pmfs $p_1,p_2$,} p(x_1,x_2)&amp;=p(x_1)p(x_2)
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-224"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.45</span></span>.       For independent discrete random variables \(X_1,X_2\), For any two sets \(A,B\subset \R \),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--



                                                                                                    P (X1 ∈ A and X2 ∈ B) = P (X1 ∈ A)P (X2 ∈ B)

In particular, for marginal cdfs \(F_1,F_2\),

                                                                                          P (X1 ≤ x1 and X2 ≤ x2 ) = P (X1 ≤ x1 )P (X2 ≤ x2 ) = F1 (x1 )F2 (x2 )



-->


<p>


\begin{gather*}
P(X_1\in A\text { and }X_2\in B)=P(X_1\in A)P(X_2\in B) \intertext {In particular, for marginal cdfs $F_1,F_2$,} P(X_1\le x_1\text { and }X_2\le x_2)=P(X_1\le x_1)P(X_2\le x_2)=F_1(x_1)F_2(x_2)
\end{gather*}


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-225"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span> For \(A=\{x_{1_1},\dots , x_{1_m}\},\ B=\{x_{2_1},\dots , x_{1_n}\}\),
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--


                                                                                                                     m n
                                                                                                                     X X
                                                                                           P (X1 ∈ A and X2 ∈ B) =             P (X1 = xi and X2 = xj )
                                                                                                                     i=1 j=1
                                                                                                                     m n
                                                                                                                     X X
                                                                                                                =              P (X1 = xi )P (X2 = xj )
                                                                                                                     i=1 j=1
                                                                                                                      m
                                                                                                                                        !    n
                                                                                                                                                              !
                                                                                                                      X                      X
                                                                                                                =           P (X1 = xi )           P (X2 = xj )
                                                                                                                      i=1                    j=1

                                                                                                                = P (X1 ∈ A)P (X2 ∈ B)



-->


<p>


\begin{align*}
P(X_1\in A\text { and }X_2\in B)&amp;=\sum _{i=1}^m\sum _{j=1}^nP(X_1=x_i\text { and }X_2=x_j)\\ &amp;=\sum _{i=1}^m\sum _{j=1}^nP(X_1=x_i)P(X_2=x_j)\\ &amp;=\left (\sum _{i=1}^mP(X_1=x_i)\right )\left (\sum _{j=1}^nP(X_2=x_j)\right )\\
&amp;=P(X_1\in A)P(X_2\in B)\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-226"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.46</span></span><span class="amsthmnotedefinition"> (Expectation)</span>.   <a id="thm:inexp"></a> For independent discrete random
variables whose expectations exist,
</p>

<p>
\[ E(X_1X_2)=E(X_1)E(X_2) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-227"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--


                                                                                                                      XX
                                                                                                      E(X1 , X2 ) =             x1 x2 p(x1 , x2 )
                                                                                                                      x1   x2
                                                                                                                      XX
                                                                                                                 =              x1 x2 p(x1 )p(x2 )
                                                                                                                      x1   x2
                                                                                                                      X                X
                                                                                                                 =         x1 p(x1 )        x2 p(x2 )
                                                                                                                      x1               x2

                                                                                                                 = E(X1 )E(X2 )



-->


<p>


\begin{align*}
E(X_1,X_2)&amp;=\sum _{x_1}\sum _{x_2}x_1x_2p(x_1,x_2)\\ &amp;=\sum _{x_1}\sum _{x_2}x_1x_2p(x_1)p(x_2)\\ &amp;=\sum _{x_1}x_1p(x_1)\sum _{x_2}x_2p(x_2)\\ &amp;=E(X_1)E(X_2)\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-228"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.47</span></span><span class="amsthmnotedefinition"> (Covariance)</span>.   For independent discrete random variables \(X_1,X_2\),
their covariance is zero.
</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-229"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--



                                                                                         Cov(X1 , X2 ) = E(X1 X2 ) − E(X1 )E(X2 )
                                                                                                     = E(X1 )E(X2 ) − E(X1 )E(X2 ) = 0



-->


<p>


\begin{align*}
\Cov (X_1,X_2)&amp;=E(X_1X_2)-E(X_1)E(X_2)\\ &amp;=E(X_1)E(X_2)-E(X_1)E(X_2)=0\qedhere
\end{align*}


</p>

</li>

</ul>

</div>
<div class="amsthmbodydefinition">

<ul style="list-style-type:none">


<li>

<a id="MATH0057-autopage-230"></a>

<p>
<span class="amsthmnamedefinition">Proposition</span><span class="amsthmnumberdefinition"> <span class="textup">1.48</span></span><span class="amsthmnotedefinition"> (Variance)</span>.   <a id="thm:invar"></a> For independent, or covariant
random variables \(X_1,X_2\),
</p>

<p>
\[ \Var (X_1+X_2)=\Var (X_1)+\Var (X_2) \]
</p>

<p>


</p>

</li>

</ul>

</div>
<div class="amsthmproof">

<a id="MATH0057-autopage-231"></a>

<ul style="list-style-type:none">


<li>

<p>
<span class="amsthmproofname">Proof.</span>
</p>
<span class="hidden"> \(\seteqnumber{0}{1.}{47}\)</span>


<!--



                                                                                         Var(X1 + X2 )
                                                                                                                       2
                                                                                       = E (X1 + X2 )2 − E(X1 + X2 )
                                                                                                                                   2
                                                                                       = E X12 + X22 + 2X1 X2 − E(X1 ) + E(X2 )

                                                                                       = E(X12 ) + E(X22 ) + 2E(X1 X2 ) − E(X1 )2 − E(X2 )2 − 2E(X1 )E(X2 )
                                                                                                                               
                                                                                       = E(X12 ) − E(X1 )2 + E(X22 ) − E(X2 )2 + 2E(X1 X2 ) − 2E(X1 X2 )

                                                                                       = Var(X1 ) + Var(X2 )



-->


<p>


\begin{align*}
&amp;\phantom {=}\Var (X_1+X_2)\\ &amp;=E\big ((X_1+X_2)^2\big )-\big (E(X_1+X_2)\big )^2\\ &amp;=E\big (X_1^2+X_2^2+2X_1X_2\big )-\big (E(X_1)+E(X_2)\big )^2\\ &amp;=E(X_1^2)+E(X_2^2)+2E(X_1X_2)-E(X_1)^2-E(X_2)^2-2E(X_1)E(X_2)\\
&amp;=\big (E(X_1^2)-E(X_1)^2\big )+\big (E(X_2^2)-E(X_2)^2\big )+2E(X_1X_2)-2E(X_1X_2)\\ &amp;=\Var (X_1)+\Var (X_2)\qedhere
\end{align*}


</p>

</li>

</ul>

</div>

<a id="MATH0057-autofile-last"></a>
